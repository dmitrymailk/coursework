{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('ai-env')",
   "metadata": {
    "interpreter": {
     "hash": "ee89ffdf677b068b3969c9c92fc557abd8fe9bdccee3c1b3432324df722c1402"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import emoji\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Медиум\n",
    "# - Гарри поттер\n",
    "# - Документация фреймворка\n",
    "# - Статьи с реддита\n",
    "# - Новости с BBC\n",
    "# - TED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_text(dirty_text):\n",
    "    bad_chars = {\n",
    "        \"’\":\"'\",\n",
    "        \"‘\": \"'\",\n",
    "        \"“\": ' ',\n",
    "        \"”\": ' ',\n",
    "        \"—\": \"-\",\n",
    "        \"…\": \"...\",\n",
    "        \"–\": \"-\",\n",
    "        '\"': \" \",\n",
    "        '[': \"\",\n",
    "        \"]\": \"\",\n",
    "        '(Laughter)': ' ',\n",
    "        '(Applause)': ' ',\n",
    "        '--': '-',\n",
    "        \"&gt;\": ''\n",
    "    }\n",
    "    new_text = str(dirty_text).strip()\n",
    "    for bad_char in bad_chars:\n",
    "        new_text = new_text.replace(bad_char, bad_chars[bad_char])\n",
    "                \n",
    "    _replace_whitespace_ = re.compile(r\"\\s+\")\n",
    "    new_text = _replace_whitespace_.sub(\" \", new_text).strip()\n",
    "\n",
    "    _replace_new_chapter = re.compile(r\"(Page \\| [\\d]{1,} Harry Potter and the Philosophers Stone - J.K. Rowling)\")\n",
    "    new_text = _replace_new_chapter.sub(\" \", new_text).strip()\n",
    "\n",
    "    _replace_multiple_whitespaces = re.compile(r'\\s{1,}')\n",
    "    new_text = _replace_multiple_whitespaces.sub(\" \", new_text).strip()\n",
    "\n",
    "    _replace_any_link_ = re.compile(r\"http[^\\s]+\")\n",
    "    new_text = _replace_any_link_.sub(\" \", new_text).strip()\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "500*0.05"
   ]
  },
  {
   "source": [
    "# - Медиум"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_dataset = pd.read_csv(\"medium-for-grammar.csv\", encoding='utf-8')\n",
    "medium_dataset = medium_dataset.drop(columns=['title', 'img', 'link', 'topic', 'uuid'])"
   ]
  },
  {
   "source": [
    "# - Гарри поттер"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "qwe\nasd\n"
     ]
    }
   ],
   "source": [
    "for item in {\"qwe\":12, 'asd':3 }:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "harry_dataset = open(\"Harry.txt\", encoding='utf-8').read()\n",
    "harry_dataset = pretty_text(harry_dataset)"
   ]
  },
  {
   "source": [
    "## - Документация фреймворка"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentations_dataset = open(\"documentation.txt\", encoding='utf-8').read()\n",
    "documentations_dataset = pretty_text(documentations_dataset)"
   ]
  },
  {
   "source": [
    "## - Статьи с реддита"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        id   parent_id subreddit_id   link_id  \\\n",
       "0  c092j8m  t1_c092gss     t5_2qh2p  t3_8eyy3   \n",
       "1  c4imcva  t1_c4im948     t5_2qh1i  t3_t0ynr   \n",
       "2  c0s4nfi  t1_c0s4lje     t5_2qh1i  t3_cf1n2   \n",
       "3  c4ini33  t1_c4incln     t5_2qh1i  t3_t0ynr   \n",
       "4  c4imgel  t1_c4ima2e     t5_2qh1i  t3_t0ynr   \n",
       "\n",
       "                                                text  score   ups  \\\n",
       "0  This isn't Twitter: try to comment on the arti...   9582  9582   \n",
       "1  Well, it is exactly what it sounds like. It's ...   9531  9531   \n",
       "2                In soviet Russia, bomb disarms you!   8545  8545   \n",
       "3                        \"runin for senitur! #YOLO!\"   7430  7430   \n",
       "4                             You step motherfucker.   7173  7173   \n",
       "\n",
       "         author  controversiality parent_link_id  \\\n",
       "0    nraustinii                 0       t3_8eyy3   \n",
       "1       Lynfect                 0       t3_t0ynr   \n",
       "2  CapnScumbone                 0       t3_cf1n2   \n",
       "3     [deleted]                 0       t3_t0ynr   \n",
       "4         jbg89                 0       t3_t0ynr   \n",
       "\n",
       "                                         parent_text  parent_score  \\\n",
       "0                                    Fucking faggot.         -7526   \n",
       "1                 Elaborate on this cum box, please.          3841   \n",
       "2  I don't live in Russia anymore, and I will not...           621   \n",
       "3  This just made me realize that future presiden...          4651   \n",
       "4  I have sex with my step mom when my dad isn't ...          4251   \n",
       "\n",
       "   parent_ups parent_author  parent_controversiality  \n",
       "0       -7526    Glorificus                        0  \n",
       "1        3841      eeeeevil                        0  \n",
       "2         621       shady8x                        0  \n",
       "3        4651       drspg99                        0  \n",
       "4        4251        audir8                        0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>parent_id</th>\n      <th>subreddit_id</th>\n      <th>link_id</th>\n      <th>text</th>\n      <th>score</th>\n      <th>ups</th>\n      <th>author</th>\n      <th>controversiality</th>\n      <th>parent_link_id</th>\n      <th>parent_text</th>\n      <th>parent_score</th>\n      <th>parent_ups</th>\n      <th>parent_author</th>\n      <th>parent_controversiality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c092j8m</td>\n      <td>t1_c092gss</td>\n      <td>t5_2qh2p</td>\n      <td>t3_8eyy3</td>\n      <td>This isn't Twitter: try to comment on the arti...</td>\n      <td>9582</td>\n      <td>9582</td>\n      <td>nraustinii</td>\n      <td>0</td>\n      <td>t3_8eyy3</td>\n      <td>Fucking faggot.</td>\n      <td>-7526</td>\n      <td>-7526</td>\n      <td>Glorificus</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>c4imcva</td>\n      <td>t1_c4im948</td>\n      <td>t5_2qh1i</td>\n      <td>t3_t0ynr</td>\n      <td>Well, it is exactly what it sounds like. It's ...</td>\n      <td>9531</td>\n      <td>9531</td>\n      <td>Lynfect</td>\n      <td>0</td>\n      <td>t3_t0ynr</td>\n      <td>Elaborate on this cum box, please.</td>\n      <td>3841</td>\n      <td>3841</td>\n      <td>eeeeevil</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>c0s4nfi</td>\n      <td>t1_c0s4lje</td>\n      <td>t5_2qh1i</td>\n      <td>t3_cf1n2</td>\n      <td>In soviet Russia, bomb disarms you!</td>\n      <td>8545</td>\n      <td>8545</td>\n      <td>CapnScumbone</td>\n      <td>0</td>\n      <td>t3_cf1n2</td>\n      <td>I don't live in Russia anymore, and I will not...</td>\n      <td>621</td>\n      <td>621</td>\n      <td>shady8x</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>c4ini33</td>\n      <td>t1_c4incln</td>\n      <td>t5_2qh1i</td>\n      <td>t3_t0ynr</td>\n      <td>\"runin for senitur! #YOLO!\"</td>\n      <td>7430</td>\n      <td>7430</td>\n      <td>[deleted]</td>\n      <td>0</td>\n      <td>t3_t0ynr</td>\n      <td>This just made me realize that future presiden...</td>\n      <td>4651</td>\n      <td>4651</td>\n      <td>drspg99</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>c4imgel</td>\n      <td>t1_c4ima2e</td>\n      <td>t5_2qh1i</td>\n      <td>t3_t0ynr</td>\n      <td>You step motherfucker.</td>\n      <td>7173</td>\n      <td>7173</td>\n      <td>jbg89</td>\n      <td>0</td>\n      <td>t3_t0ynr</td>\n      <td>I have sex with my step mom when my dad isn't ...</td>\n      <td>4251</td>\n      <td>4251</td>\n      <td>audir8</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "reddit_dataset = pd.read_csv(\"reddit_comments_positive.csv\", encoding='utf-8')\n",
    "# reddit_dataset = reddit_dataset.iloc[:2000]\n",
    "# reddit_dataset = reddit_dataset.drop(columns=['id', 'parent_id', \n",
    "#                                     'subreddit_id', \n",
    "#                                     'link_id', \n",
    "#                                     'score',\n",
    "#                                     'ups',\n",
    "#                                     'author',\n",
    "#                                     'controversiality',\n",
    "#                                     'parent_link_id',\n",
    "#                                     'parent_score',\n",
    "#                                     'parent_ups',\n",
    "#                                     'parent_author',\n",
    "#                                     'parent_controversiality'\n",
    "#                                     ])\n",
    "reddit_dataset[reddit_dataset['score'] > 500].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_dataset = reddit_dataset[reddit_dataset['score'] > 500]\n",
    "reddit_dataset = reddit_dataset[['text', 'parent_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"Magic, got it. They're made using a couple of MEMS (microelectromechanical systems(  processes, on top of a plane (called a substrate, made of silicon, usually). One is called bulk micromachining, where the substrate is etched into during production. This is done by a process called photolithography(  The other is called surface micromachining, where thin layers of the fabrication material (mostly silicon, I believe) are piled on top of the substrate to create the shape, and other bits are etched in. Etching is done using processes like photolithography(  among others - using a surface material that changes its properties to be soluble on being exposed to light. If the light is applied in a shape, you can make that shape be soluble; that can be dissolved in something called a developer, leaving the inverse of that shape in the material. That masking material then protects the silicon below in that exact shape from whatever actual etching process you use: nitric acid is an example. The specific service that they use for both is called PolyMUMPs(  Edit: the site seems to have been hugged to death, so the relevant content: **PolyMUMPs** ... Many universities use the service today as a way to teach beginning MEMS design at the undergraduate level, using PolyMUMPs as the example process. In this environment, classes will study MUMPs® during the semester then submit a design and get chips back in time for the end of the semester. Another use for PolyMUMPs is as a benchmarking tool for software models and statistical studies, where theories are validated with measured data from actual, fabricated chips. PolyMUMPs chips also serve as a standard building block for larger scale systems, where the MEMS chip is only one piece of the overall system. PolyMUMPs' well-understood properties and predictable outcomes are advantageously leveraged to make MEMS the black box fixed-component. **About The Process** PolyMUMPs is a three-layer polysilicon surface and bulk micromachining process, with 2 sacrificial layers and one metal layer. Eight mask levels create 7 physical layers. The minimum feature size in PolyMUMPs is 2um. Devices that can be made in PolyMUMPs include: Acoustics(microphones), Sensors, Accelerometers, Micro-fluidics, and Display Technologies. *** Here's a GIF(  of it being used; I also posted(/r/gifs/comments/2xl4or/this_is_a_nanoinjector_a_microscopic_machine/) it to /r/gifs. Here's a video about it(\""
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "def convert_reddit(row):\n",
    "    parent_text = row['parent_text']\n",
    "    text = row['text']\n",
    "    result = pretty_text(text) + \" \" + pretty_text(parent_text)\n",
    "    return result\n",
    "\n",
    "convert_reddit(reddit_dataset.iloc[207])"
   ]
  },
  {
   "source": [
    "## - Новости "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             content\n",
       "0  The son of a Louisiana man whose father was sh...\n",
       "1  Copies of William Shakespeare’s first four boo...\n",
       "2  Debt: $20, 000, Source: College, credit cards,...\n",
       "3  It was late. I was drunk, nearing my 35th birt...\n",
       "4  A central Texas man serving a life sentence fo..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The son of a Louisiana man whose father was sh...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Copies of William Shakespeare’s first four boo...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Debt: $20, 000, Source: College, credit cards,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>It was late. I was drunk, nearing my 35th birt...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A central Texas man serving a life sentence fo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "news_dataset = pd.read_csv(\"news.csv\", encoding='utf-8')\n",
    "news_dataset = news_dataset.drop(columns=[\n",
    "                                    'id', \n",
    "                                    'title', \n",
    "                                    'publication', \n",
    "                                    'author',\n",
    "                                    'date',\n",
    "                                    'year',\n",
    "                                    'month',\n",
    "                                    'url',\n",
    "                                    'Unnamed: 0'\n",
    "                                    ])\n",
    "news_dataset.head()"
   ]
  },
  {
   "source": [
    "## - TED"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                text\n",
       "0  Thank you so much, Chris. And it's truly a gre...\n",
       "1  In terms of invention, I'd like to tell you th...\n",
       "2  A public, Dewey long ago observed, is constitu...\n",
       "3  I want to start off by saying, Houston, we hav...\n",
       "4  What I want to talk about is, as background, i..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Thank you so much, Chris. And it's truly a gre...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In terms of invention, I'd like to tell you th...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A public, Dewey long ago observed, is constitu...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I want to start off by saying, Houston, we hav...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What I want to talk about is, as background, i...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "ted_dataset = pd.read_csv(\"ted-dataframe.csv\", encoding='utf-8')\n",
    "ted_dataset = ted_dataset.drop(columns=[\n",
    "                                    'talk_name', \n",
    "                                    'talk_url', \n",
    "                                    'talk_url_transcript', \n",
    "                                    'ted_number',\n",
    "                                    ])\n",
    "ted_dataset.head()"
   ]
  },
  {
   "source": [
    "## Create dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_dataframe = pd.DataFrame(columns=['sent', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = {\n",
    "    'news': news_dataset,\n",
    "    'ted': ted_dataset,\n",
    "    'reddit': reddit_dataset,\n",
    "    'harry': harry_dataset,\n",
    "    'medium': medium_dataset,\n",
    "    'documentation': documentations_dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "[1, 2, 3][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokinizer(text, max_length=25, min_length=12):\n",
    "    text = pretty_text(text)\n",
    "    text = str(text)\n",
    "    doc = nlp(text)\n",
    "    sentences = []\n",
    "    for sent in doc.sents:\n",
    "        sent = str(sent)\n",
    "        doc_sent = nlp(sent)\n",
    "        if (len(doc_sent) > min_length \n",
    "        and len(doc_sent) < max_length \n",
    "        and sent[0].isalpha() \n",
    "        and not sent[-1].isalpha()\n",
    "        and sent[0].isupper()  \n",
    "        and not '...' in sent \n",
    "        and not '*' in sent\n",
    "        and not '/' in sent\n",
    "        and not '(' in sent\n",
    "        and not ')' in sent\n",
    "        and not bool(emoji.get_emoji_regexp().search(sent))) \\\n",
    "        and not sent[-1] in [':', \",\" \"-\"]:\n",
    "            # print(f\"{len(doc_sent)} - {sent}\")\n",
    "            sentences.append(sent)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = news_dataset['content'].apply(pretty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_dataset = ted_dataset['text'].apply(pretty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_dataset = [convert_reddit(reddit_dataset.iloc[i]) for i in range(len(reddit_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_dataset = medium_dataset['text'].apply(pretty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_tokinizer(medium_dataset.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_tokinizer(news_dataset.iloc[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\",\n",
       " 'Now I have to take off my shoes or boots to get on an airplane!',\n",
       " \"I'll tell you one quick story to illustrate what that's been like for me.\",\n",
       " \"It's a true story - every bit of this is true.\",\n",
       " 'White House - we were driving from our home in Nashville to a little farm we have 50 miles east of Nashville.',\n",
       " 'It was dinnertime, and we started looking for a place to eat.',\n",
       " \"We got off the exit, we found a Shoney's restaurant.\",\n",
       " \"Low-cost family restaurant chain, for those of you who don't know it.\",\n",
       " 'We went in and sat down at the booth, and the waitress came over, made a big commotion over Tipper.',\n",
       " \"And she said Yes, that's former Vice President Al Gore and his wife, Tipper.\",\n",
       " \"And the man said, He's come down a long way, hasn't he?\",\n",
       " 'And I began the speech by telling them the story of what had just happened the day before in Nashville.',\n",
       " \"And I told it pretty much the same way I've just shared it with you: Tipper and I were driving ourselves,\",\n",
       " \"Shoney's, low-cost family restaurant chain, what the man said - they laughed.\",\n",
       " 'I gave my speech, then went back out to the airport to fly back home.',\n",
       " 'I fell asleep on the plane until, during the middle of the night, we landed on the Azores Islands for refueling.',\n",
       " 'And he was waving a piece of paper, and he was yelling, Call Washington!',\n",
       " \"I have opened a low-cost family restaurant' - 'named Shoney's, and we are running it ourselves.'\",\n",
       " 'I add new images, because I learn more about it every time I give it.',\n",
       " 'Every time the tide comes in and out, you find some more shells.',\n",
       " 'Just in the last two days, we got the new temperature records in January.',\n",
       " 'Historical average for Januarys is 31 degrees; last month was 39.5 degrees.',\n",
       " \"But these are the recapitulation slides, and then I'm going to go into new material about what you can do.\",\n",
       " 'Efficiency in end-use electricity and end-use of all energy is the low-hanging fruit.',\n",
       " 'Cars and trucks - I talked about that in the slideshow, but I want you to put it in perspective.',\n",
       " 'Cars and trucks are very significant, and we have the lowest standards in the world.',\n",
       " 'Renewables at the current levels of technological efficiency can make this much difference.',\n",
       " 'Consider this: Make a decision to live a carbon-neutral life.',\n",
       " 'A lot of us in here have made that decision, and it is really pretty easy.',\n",
       " 'You can very precisely calculate what your CO2 emissions are, and then you will be given options to reduce.',\n",
       " \"Again, some of us have done that, and it's not as hard as you think.\",\n",
       " \"The movie is a movie version of the slideshow I gave two nights ago, except it's a lot more entertaining.\",\n",
       " 'Many of you here have the opportunity to ensure that a lot of people see it.',\n",
       " \"Where did anybody get the idea that you ought to stay arm's length from politics?\",\n",
       " \"It doesn't mean that if you're a Republican, that I'm trying to convince you to be a Democrat.\",\n",
       " 'This used to be a bipartisan issue, and I know that in this group it really is.',\n",
       " 'Support the idea of capping carbon dioxide emissions - global warming pollution - and trading it.',\n",
       " \"Here's why: as long as the United States is out of the world system, it's not a closed system.\",\n",
       " 'The market will work to solve this problem - if we can accomplish this.',\n",
       " 'Because presently, the politicians do not have permission to do what needs to be done.',\n",
       " \"It's now repetition of short, hot-button, 30-second, 28-second television ads.\",\n",
       " \"Let's re-brand global warming, as many of you have suggested.\",\n",
       " \"I said the other night, and I'll repeat now: this is not a political issue.\",\n",
       " 'You have more influence than some of us who are Democrats do.',\n",
       " 'Not just this, but connected to the ideas that are here, to bring more coherence to them.']"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "sent_tokinizer(ted_dataset.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['He told me he dreams in English because my mom only speaks English and his dreams are exclusively of fucking my mom.',\n",
       " 'That actually happened, later in life, but now I dream in english.',\n",
       " 'When I learned to speak english fluently, my dreams changed to english.']"
      ]
     },
     "metadata": {},
     "execution_count": 284
    }
   ],
   "source": [
    "# sent_tokinizer(reddit_dataset[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 309
    }
   ],
   "source": [
    " '...' in ''"
   ]
  },
  {
   "source": [
    "## Add news"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(70):\n",
    "    text = news_dataset.iloc[i]\n",
    "    sents = sent_tokinizer(text)\n",
    "    if len(sents) > 0:\n",
    "        for sent in sents:\n",
    "            row = {}\n",
    "            row['sentence'] = sent\n",
    "            row['source'] = 'news'\n",
    "            texts_dataframe = texts_dataframe.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1006"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "len(texts_dataframe)"
   ]
  },
  {
   "source": [
    "## Add Ted"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(70):\n",
    "    text = ted_dataset.iloc[i]\n",
    "    sents = sent_tokinizer(text)\n",
    "    if len(sents) > 0:\n",
    "        for sent in sents:\n",
    "            row = {}\n",
    "            row['sentence'] = sent\n",
    "            row['source'] = 'ted'\n",
    "            texts_dataframe = texts_dataframe.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5488"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "len(texts_dataframe)"
   ]
  },
  {
   "source": [
    "## Add Reddit"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-bf7298e5910d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreddit_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreddit_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_tokinizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-99e57b5f63bb>\u001b[0m in \u001b[0;36msent_tokinizer\u001b[1;34m(text, max_length, min_length)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mdoc_sent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         if (len(doc_sent) > min_length \n\u001b[0;32m      9\u001b[0m         \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_sent\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\AI\\ai-env\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__call__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32md:\\AI\\ai-env\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\AI\\ai-env\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32md:\\AI\\ai-env\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\AI\\ai-env\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(seqs_in, drop)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\AI\\ai-env\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\AI\\ai-env\\lib\\site-packages\\thinc\\neural\\_classes\\resnet.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbp_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\AI\\ai-env\\lib\\site-packages\\thinc\\neural\\_classes\\feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\AI\\ai-env\\lib\\site-packages\\thinc\\neural\\_classes\\layernorm.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X, drop)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchild\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackprop_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mbackprop_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\AI\\ai-env\\lib\\site-packages\\thinc\\neural\\_classes\\maxout.py\u001b[0m in \u001b[0;36mbegin_update\u001b[1;34m(self, X__bi, drop)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdrop\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mdrop\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_factor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0moutput__boc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX__bi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0moutput__boc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnO\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0moutput__boc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput__boc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput__boc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    text = reddit_dataset[i]\n",
    "    sents = sent_tokinizer(text)\n",
    "    if len(sents) > 0:\n",
    "        for sent in sents:\n",
    "            row = {}\n",
    "            row['sentence'] = sent\n",
    "            row['source'] = 'reddit'\n",
    "            texts_dataframe = texts_dataframe.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "11832"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "len(texts_dataframe)"
   ]
  },
  {
   "source": [
    "## Add harry "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "harry_sents = sent_tokinizer(harry_dataset)\n",
    "if len(harry_sents) > 0:\n",
    "    for sent in harry_sents:\n",
    "        row = {}\n",
    "        row['sentence'] = sent\n",
    "        row['source'] = 'harry'\n",
    "        texts_dataframe = texts_dataframe.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "13770"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "len(texts_dataframe)"
   ]
  },
  {
   "source": [
    "## Add medium"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "len(medium_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Franz Kafka moved in with me today. His hair is greasy with the slime of the grave. His bald pate, colored like a palm tree in the middle of an oasis of hair, shakes dandruff sequins from the desert mirage onto the floor. His hollow eyes, the size of a vulture\\'s balls, penetrate me.\"Franz,\" I say, \"what are you doing at an hour like this? You ought to be in bed!\"Franz winks and smiles wide, revealing what remains of his two teeth. They are the color of ointment extracted from a baby\\'s behind three days after its deposit.\"Franz, why don\\'t you talk to me? Speak. That\\'s what you were born to do.\"\"I\\'ve been talking all my life,\" he says, \"but no one listened. In death it\\'s too late to speak.\"After the fact. Poor boy. He never looked like a man. There he sits, in a Halloween costume: a white sheet with boogie monster eye-slits beneath his forehead, trapped inside a boy\\'s body, frozen at the age of twenty-two.\"Franz, I feel sorry for you. You\\'re stuck inside a boy\\'s body. You look twenty two.\"\"If you\\'d let me marry you,\" he continues with my rhyme, \"I wouldn\\'t be so blue.\"\"That\\'s cheap. You can do better than that. Stick to prose in the future. We\\'ve made great strides since your time in the realm of poetry, and you don\\'t stand a chance of catching up with us.\"\"And what about prose?\" he asks.\"The novel - alas the novel! - always lags behind. You\\'re still ahead of us in that area, bro.\"He seems to think it strange that I speak with him on such familiar terms, my being a woman and all, but I explain to him the sexual revolution and finally comprehension\\'s light bulb flickers.\"It\\'s simple,\" I say. \"The world has changed in unfathomable ways since you were born. War didn\\'t even have a shape back then. Now it resembles a blob of chewing gum that expands as you add one more blob to the mixture. We keep chewing, chewing, chewing, until some nasty fink comes along and rips it out of our mouth.\"\"When will that happen?\" he asks.\"When the world ends,\" I say. \"Nuclear war.\"\"How come you know all this?\" he asks. \"How can you sound so certain?\"\"It takes practice, bud.\" I lean against him to make him smell the beer on my tongue, the froth still fresh on my lips. \"I have a talent for truth.\"\"Women were certainly different in my day and age. They were more delicate. I liked them better then.\"\"Me too,\" I say.Franz keeps me up, in the middle of night, refusing me sleep. I tell him that I deserve sleep, having worked all my life. He disagrees. Posterity always wins. Franz is a talker. I\\'m a listener. His steady voice grates heavy on my ear, an ax that hurts so much that finally I ask him what\\'s the point.\"The point of what?\" he says, confused. This is the first time that I have interrupted him in the middle of a speech.\"The point of writing,\" I explain. \"The point if it doesn\\'t redeem your pain.\"\"Easy,\" he says. \"The books we need are the kind that act upon us like a misfortune. They make us suffer like the death of someone we love more than ourselves. They make us feel as though we were on the verge of suicide - \"\"Franz,\" I interrupt again, worried, \"why do you say that?\" He ignores me.\" - lost in a forest remote from human habitation. A book should serve as an ax for the frozen sea within us.\"Now that he\\'s coined a phrase I don\\'t think he\\'ll ever shut up. Franz\\'s voice is a lonely trumpet in the middle of the sea, but his loneliness is a matter of indifference to him, as well as, perhaps, to me. Franz keeps talking.Join our free newsletter & keep up to date with new authors and help readers & writers connect!'"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "medium_dataset.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(62, len(medium_dataset)):\n",
    "    text = medium_dataset.iloc[i]['text']\n",
    "    sents = sent_tokinizer(text)\n",
    "    if len(sents) > 0:\n",
    "        for sent in sents:\n",
    "            row = {}\n",
    "            row['sent'] = sent\n",
    "            row['type'] = ''\n",
    "            texts_dataframe = texts_dataframe.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_dataframe.to_csv(\"medium_2_4_21.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_dataframe.to_csv('combined_texts.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                sentence  source\n",
       "0      I came to talk to everyone about one: the deat...    news\n",
       "1      People, in general, no matter what the race, s...    news\n",
       "2      Yes, you can protest, but I want everyone to p...    news\n",
       "3      Protest in peace - no guns, no drugs, no alcoh...    news\n",
       "4      The police in Dallas, Texas, they didn't deser...    news\n",
       "...                                                  ...     ...\n",
       "15618  Our land has become too much like jail, so we ...  medium\n",
       "15619  He might look to the ocean next, for more than...  medium\n",
       "15620  What must it feel like to begin a sojourn to a...  medium\n",
       "15621       You don't know what it's like, how it feels.  medium\n",
       "15622  I feel connected to my Bantu ancestors, my old...  medium\n",
       "\n",
       "[15623 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I came to talk to everyone about one: the deat...</td>\n      <td>news</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>People, in general, no matter what the race, s...</td>\n      <td>news</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Yes, you can protest, but I want everyone to p...</td>\n      <td>news</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Protest in peace - no guns, no drugs, no alcoh...</td>\n      <td>news</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The police in Dallas, Texas, they didn't deser...</td>\n      <td>news</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15618</th>\n      <td>Our land has become too much like jail, so we ...</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>15619</th>\n      <td>He might look to the ocean next, for more than...</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>15620</th>\n      <td>What must it feel like to begin a sojourn to a...</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>15621</th>\n      <td>You don't know what it's like, how it feels.</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>15622</th>\n      <td>I feel connected to my Bantu ancestors, my old...</td>\n      <td>medium</td>\n    </tr>\n  </tbody>\n</table>\n<p>15623 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "texts_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "105.0"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "525 / 5"
   ]
  },
  {
   "source": [
    "# Split combined data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_texts = pd.read_csv('combined_texts.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15623"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "len(combined_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               sentence source\n",
       "1006  And it's truly a great honor to have the oppor...    ted\n",
       "1007  Now I have to take off my shoes or boots to ge...    ted\n",
       "1008  I'll tell you one quick story to illustrate wh...    ted\n",
       "1009     It's a true story - every bit of this is true.    ted\n",
       "1010  White House - we were driving from our home in...    ted\n",
       "...                                                 ...    ...\n",
       "5483  We've got to make something better than what w...    ted\n",
       "5484  Just to wrap up, in the immortal words of H.G....    ted\n",
       "5485  I think that, in fact, that all of the past is...    ted\n",
       "5486  All that the human mind has accomplished is bu...    ted\n",
       "5487  The people in this room have given me more con...    ted\n",
       "\n",
       "[4482 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1006</th>\n      <td>And it's truly a great honor to have the oppor...</td>\n      <td>ted</td>\n    </tr>\n    <tr>\n      <th>1007</th>\n      <td>Now I have to take off my shoes or boots to ge...</td>\n      <td>ted</td>\n    </tr>\n    <tr>\n      <th>1008</th>\n      <td>I'll tell you one quick story to illustrate wh...</td>\n      <td>ted</td>\n    </tr>\n    <tr>\n      <th>1009</th>\n      <td>It's a true story - every bit of this is true.</td>\n      <td>ted</td>\n    </tr>\n    <tr>\n      <th>1010</th>\n      <td>White House - we were driving from our home in...</td>\n      <td>ted</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5483</th>\n      <td>We've got to make something better than what w...</td>\n      <td>ted</td>\n    </tr>\n    <tr>\n      <th>5484</th>\n      <td>Just to wrap up, in the immortal words of H.G....</td>\n      <td>ted</td>\n    </tr>\n    <tr>\n      <th>5485</th>\n      <td>I think that, in fact, that all of the past is...</td>\n      <td>ted</td>\n    </tr>\n    <tr>\n      <th>5486</th>\n      <td>All that the human mind has accomplished is bu...</td>\n      <td>ted</td>\n    </tr>\n    <tr>\n      <th>5487</th>\n      <td>The people in this room have given me more con...</td>\n      <td>ted</td>\n    </tr>\n  </tbody>\n</table>\n<p>4482 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "combined_texts[combined_texts['source'] == 'ted'].iloc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_dataset = pd.read_csv('multi_label_tenses.csv', encoding='utf-8',)\n",
    "prev_dataset['sentence'] = prev_dataset['sent']\n",
    "prev_dataset['source'] = \"other\"\n",
    "combined_texts = pd.concat([combined_texts, prev_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16398"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "len(combined_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16398"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "len(combined_texts['sentence'][\n",
    "    ((combined_texts['sentence'][combined_texts['source'] == 'other'].sort_index(inplace=True)) != combined_texts['sentence'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_domains = ['news', 'reddit', 'harry', 'medium', 'ted']\n",
    "balanced_combined_texts = pd.DataFrame(columns=['sentence', 'source'])\n",
    "for sub_domain in texts_domains:\n",
    "    sub_dataframe = combined_texts[\n",
    "        (combined_texts['source'] == sub_domain)\n",
    "        &\n",
    "        ((combined_texts['sentence'][combined_texts['source'] == 'other'].sort_index(inplace=True)) \n",
    "        != combined_texts['sentence'])\n",
    "    ].sample(n=125, random_state=5)\n",
    "    balanced_combined_texts = pd.concat([balanced_combined_texts, sub_dataframe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_combined_texts[['sentence', 'source']].to_csv('balanced_combined_texts_v3.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "len(balanced_combined_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_combined_texts = pd.read_csv(\"balanced_combined_texts_v3.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              sentence source\n",
       "0    And maybe, just maybe, you felt one candidate ...   news\n",
       "1    Speaking on the campaign trail, the Republican...   news\n",
       "2    They do things like make big shots and do some...   news\n",
       "3    But Trump only has one shot left to do that: t...   news\n",
       "4    Some of the fondest memories I have of her are...   news\n",
       "..                                                 ...    ...\n",
       "620  Everything is hopeless - we're always being to...    ted\n",
       "621  We leveraged that $10,000 seed grant more than...    ted\n",
       "622  This pro-aging trance is what stops us from ag...    ted\n",
       "623  You can engineer a prairie vole to become mono...    ted\n",
       "624  As you can see, I can use the same two-fingere...    ted\n",
       "\n",
       "[625 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>And maybe, just maybe, you felt one candidate ...</td>\n      <td>news</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Speaking on the campaign trail, the Republican...</td>\n      <td>news</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>They do things like make big shots and do some...</td>\n      <td>news</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>But Trump only has one shot left to do that: t...</td>\n      <td>news</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Some of the fondest memories I have of her are...</td>\n      <td>news</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>620</th>\n      <td>Everything is hopeless - we're always being to...</td>\n      <td>ted</td>\n    </tr>\n    <tr>\n      <th>621</th>\n      <td>We leveraged that $10,000 seed grant more than...</td>\n      <td>ted</td>\n    </tr>\n    <tr>\n      <th>622</th>\n      <td>This pro-aging trance is what stops us from ag...</td>\n      <td>ted</td>\n    </tr>\n    <tr>\n      <th>623</th>\n      <td>You can engineer a prairie vole to become mono...</td>\n      <td>ted</td>\n    </tr>\n    <tr>\n      <th>624</th>\n      <td>As you can see, I can use the same two-fingere...</td>\n      <td>ted</td>\n    </tr>\n  </tbody>\n</table>\n<p>625 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "balanced_combined_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sentence, source]\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "balanced_combined_texts[balanced_combined_texts.duplicated() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      And maybe, just maybe, you felt one candidate ...\n",
       "1      Speaking on the campaign trail, the Republican...\n",
       "2      They do things like make big shots and do some...\n",
       "3      But Trump only has one shot left to do that: t...\n",
       "4      Some of the fondest memories I have of her are...\n",
       "                             ...                        \n",
       "770    And the fact that they wanted to support the c...\n",
       "771    And you reverse the pressure on your membrane ...\n",
       "772    I don't want to be part of any mainstream move...\n",
       "773    This is terrific news and will surely create m...\n",
       "774    Their goal wasn't for you to get patted down i...\n",
       "Name: sentence, Length: 1400, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "balanced_combined_texts_cmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sent, type]\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sent</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "source": [
    "prev_dataset = pd.read_csv('multi_label_tenses.csv', encoding='utf-8',)\n",
    "prev_dataset['type'] = 'other'\n",
    "balanced_combined_texts = pd.read_csv(\"balanced_combined_texts_v3.csv\", encoding='utf-8')\n",
    "balanced_combined_texts = balanced_combined_texts.rename(columns={'sentence':'sent', 'source':'type'})\n",
    "balanced_combined_texts_cmb = pd.concat([prev_dataset, balanced_combined_texts])\n",
    "balanced_combined_texts_cmb\n",
    "balanced_combined_texts_cmb[(balanced_combined_texts_cmb['sent'].duplicated() == True) ]\n",
    "# balanced_combined_texts_cmb[(balanced_combined_texts_cmb['sent'].duplicated() == False) & (balanced_combined_texts_cmb['type'] != 'other')].to_csv(\"balanced_combined_texts_v3.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_dataset = pd.read_csv('multi_label_tenses.csv', encoding='utf-8',)\n",
    "# prev_dataset = prev_dataset['sent']\n",
    "prev_dataset[prev_dataset['sent'].duplicated() == False].to_csv('multi_label_tenses.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "source": [
    "# Clean NER data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               title  \\\n",
       "0  You're Experiencing Future Shock - It's Why Yo...   \n",
       "1  Our Over-Quest for Optimization, and the Etern...   \n",
       "2  The Impatient Person's Guide To Creating Memor...   \n",
       "\n",
       "                                                text  \\\n",
       "0  I used to look forward to the future.Now it pa...   \n",
       "1  Picked my friend up at the airport yesterday. ...   \n",
       "2  Tell me if this rings a bell: You find a new t...   \n",
       "\n",
       "                                                 img  \\\n",
       "0                                                NaN   \n",
       "1  https://miro.medium.com/max/3072/1*BIe00qRjKWx...   \n",
       "2  https://miro.medium.com/max/3840/1*VEgDJTstleD...   \n",
       "\n",
       "                                                link          topic  \\\n",
       "0  https://medium.com/curious/youre-experiencing-...  mental-health   \n",
       "1  https://medium.com/curious/our-over-quest-for-...   productivity   \n",
       "2  https://max-phillips.medium.com/the-impatient-...    mindfulness   \n",
       "\n",
       "                                                uuid  \n",
       "0  0a7b5bd25a20b1f341d9db876e76ca55d99ebf753ee6c0...  \n",
       "1  0a7b0357ee4351631884ac3ad2633727e9376ab52462eb...  \n",
       "2  0a551c4ea099c1aa52ec4474c4057f738510a1db1e076e...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n      <th>img</th>\n      <th>link</th>\n      <th>topic</th>\n      <th>uuid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>You're Experiencing Future Shock - It's Why Yo...</td>\n      <td>I used to look forward to the future.Now it pa...</td>\n      <td>NaN</td>\n      <td>https://medium.com/curious/youre-experiencing-...</td>\n      <td>mental-health</td>\n      <td>0a7b5bd25a20b1f341d9db876e76ca55d99ebf753ee6c0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Our Over-Quest for Optimization, and the Etern...</td>\n      <td>Picked my friend up at the airport yesterday. ...</td>\n      <td>https://miro.medium.com/max/3072/1*BIe00qRjKWx...</td>\n      <td>https://medium.com/curious/our-over-quest-for-...</td>\n      <td>productivity</td>\n      <td>0a7b0357ee4351631884ac3ad2633727e9376ab52462eb...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Impatient Person's Guide To Creating Memor...</td>\n      <td>Tell me if this rings a bell: You find a new t...</td>\n      <td>https://miro.medium.com/max/3840/1*VEgDJTstleD...</td>\n      <td>https://max-phillips.medium.com/the-impatient-...</td>\n      <td>mindfulness</td>\n      <td>0a551c4ea099c1aa52ec4474c4057f738510a1db1e076e...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "ner_dataset = pd.read_csv(\"./NER/medium_for_NER_2_14_21.csv\")\n",
    "ner_dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset['text'] = ner_dataset['text'].apply(pretty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "len(ner_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset_cleaned = pd.DataFrame(columns=['sent', 'source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ner_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ner_dataset)):\n",
    "    text = ner_dataset['text'][i]\n",
    "    source = ner_dataset['topic'][i]\n",
    "    sents = sent_tokinizer(text)\n",
    "    if len(sents) > 0:\n",
    "        for sent in sents:\n",
    "            row = {}\n",
    "            row['sent'] = sent\n",
    "            row['source'] = source\n",
    "            ner_dataset_cleaned = ner_dataset_cleaned.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sents = sent_tokinizer(kafka_dataset)\n",
    "if len(kafka_sents) > 0:\n",
    "    for sent in kafka_sents:\n",
    "        row = {}\n",
    "        row['sent'] = sent\n",
    "        row['source'] = 'kafka'\n",
    "        ner_dataset_cleaned = ner_dataset_cleaned.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset_cleaned.to_csv(\"./NER/medium_for_NER_2_14_21_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_dataset = open(\"./NER/kafka.txt\", encoding='utf-8').read()\n",
    "kafka_dataset = pretty_text(kafka_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset_cleaned = pd.read_csv(\"./NER/medium_for_NER_2_14_21_cleaned.csv\", encoding='utf-8')"
   ]
  },
  {
   "source": [
    "# Delete dublicates"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset_cleaned = ner_dataset_cleaned[ner_dataset_cleaned['sent'].duplicated() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sent, source]\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sent</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "ner_dataset_cleaned[ner_dataset_cleaned['sent'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset_cleaned = ner_dataset_cleaned[['sent']]\n",
    "ner_dataset_cleaned['part'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_dataset = pd.read_csv(\"./NER/combined_04_02_2021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sent, type]\n",
       "Index: []"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sent</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "prev_dataset[prev_dataset['sent'].duplicated() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_dataset = prev_dataset[['sent']]\n",
    "prev_dataset['part'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_datasets = pd.concat([prev_dataset, ner_dataset_cleaned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_datasets = combined_datasets[combined_datasets['sent'].duplicated() == False]\n",
    "combined_datasets = combined_datasets[combined_datasets['part'] == 1]\n",
    "combined_datasets = combined_datasets['sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_datasets.to_csv(\"./NER/medium_for_NER_2_14_21_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}