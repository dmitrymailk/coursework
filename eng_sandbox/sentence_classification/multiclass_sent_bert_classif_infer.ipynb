{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('ai-env')",
   "metadata": {
    "interpreter": {
     "hash": "ee89ffdf677b068b3969c9c92fc557abd8fe9bdccee3c1b3432324df722c1402"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x29749ef8c50>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "SEED = 10\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "source": [
    "# Prepare Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_texts = pd.read_csv(\"../DataLabeling/combined_texts.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            sentence source\n",
       "0  The son of a Louisiana man whose father was sh...   news\n",
       "1  Cameron Sterling, the son of Alton Sterling, w...   news\n",
       "2  Alton Sterling was killed by Baton Rouge polic...   news\n",
       "3  Baton Rouge police said in a statement that po...   news\n",
       "4  The press conference on Wednesday is Cameron's...   news"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>source</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The son of a Louisiana man whose father was sh...</td>\n      <td>news</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cameron Sterling, the son of Alton Sterling, w...</td>\n      <td>news</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Alton Sterling was killed by Baton Rouge polic...</td>\n      <td>news</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Baton Rouge police said in a statement that po...</td>\n      <td>news</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The press conference on Wednesday is Cameron's...</td>\n      <td>news</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "combined_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENT_TYPES = {'present_continuous': 0,\n",
    "            'to_be_future': 1,\n",
    "            'past_continuous': 2,\n",
    "            'past_simple': 3,\n",
    "            'can': 4,\n",
    "            'future_simple': 5,\n",
    "            'to_be_present': 6,\n",
    "            'used_to': 7,\n",
    "            'to_be_past': 8,\n",
    "            'present_simple': 9,\n",
    "            'other': 10,\n",
    "            'present_perfect': 11,\n",
    "            'could': 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'model': {\n",
    "        'max_seq_length': 128,\n",
    "        'model_name': 'distilbert-base-uncased',\n",
    "        'num_classes': 13\n",
    "    },\n",
    "    'data': {\n",
    "        'text_field_name': 'sentence',\n",
    "        'label_field_name': 'type',\n",
    "        'path_to_dataset': '../DataLabeling/combined_texts.csv',\n",
    "        'path_to_test_pred_scores': 'data/pred_present_simple.txt'\n",
    "    },\n",
    "     'training': {\n",
    "        'learn_rate': 1e-5,\n",
    "        'num_epochs': 3,                          \n",
    "        'accum_steps': 2,                         \n",
    "        'batch_size': 64,                         \n",
    "        'log_dir': 'logdir' \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_infer_data(params):\n",
    "    dataset = pd.read_csv(params['data']['path_to_dataset'], encoding='utf-8')\n",
    "    # dataset = dataset.sample(frac=1)\n",
    "    test = dataset  \n",
    "    test.to_csv('test.csv', encoding='utf-8', index=False)\n",
    "    # creating PyTorch Datasets\n",
    "\n",
    "    test_dataset = TextInferDataset(\n",
    "        texts=test[params[\"data\"][\"text_field_name\"]].values.tolist(),\n",
    "        # labels=test[params[\"data\"][\"label_field_name\"]].values,\n",
    "        max_seq_length=params[\"model\"][\"max_seq_length\"],\n",
    "        model_name=params[\"model\"][\"model_name\"],\n",
    "    )\n",
    "\n",
    "    set_global_seed(SEED)\n",
    "\n",
    "    test_loaders = {\n",
    "        \"test\": DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=params[\"training\"][\"batch_size\"],\n",
    "            shuffle=False,\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from catalyst.utils import set_global_seed\n",
    "import logging\n",
    "from transformers import AutoConfig, AutoModel\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextInferDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts,\n",
    "        max_seq_length = 128,\n",
    "        model_name = \"distilbert-base-uncased\",\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # suppresses tokenizer warnings\n",
    "        logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.FATAL)\n",
    "\n",
    "        \n",
    "        self.sep_vid = self.tokenizer.vocab[\"[SEP]\"]\n",
    "        self.cls_vid = self.tokenizer.vocab[\"[CLS]\"]\n",
    "        self.pad_vid = self.tokenizer.vocab[\"[PAD]\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # encoding the text\n",
    "        x = self.texts[index]\n",
    "\n",
    "        # a dictionary with `input_ids` and `attention_mask` as keys\n",
    "        output_dict = self.tokenizer.encode_plus(\n",
    "            x,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_seq_length,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        # for Catalyst, there needs to be a key called features\n",
    "        output_dict[\"features\"] = output_dict[\"input_ids\"].squeeze(0)\n",
    "        del output_dict[\"input_ids\"]\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "source": [
    "# Infer "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import SupervisedRunner\n",
    "from catalyst.dl.callbacks import (\n",
    "    CheckpointCallback,\n",
    "    InferCallback,\n",
    ")\n",
    "from catalyst.utils import prepare_cudnn, set_global_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loaders = prepare_infer_data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter(test_loaders['test']).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified version of the same class by HuggingFace.\n",
    "    See transformers/modeling_distilbert.py in the transformers repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, pretrained_model_name, num_classes = None, dropout = 0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            pretrained_model_name, num_labels=num_classes\n",
    "        )\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(pretrained_model_name, config=config)\n",
    "        # self.classifier = nn.Linear(config.hidden_size, num_classes)\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        self.classifier = nn.Linear(config.dim, num_classes)\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "\n",
    "    def forward(self, features, attention_mask=None, head_mask=None):\n",
    "        \"\"\"Compute class probabilities for the input sequence.\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): ids of each token,\n",
    "                size ([bs, seq_length]\n",
    "            attention_mask (torch.Tensor): binary tensor, used to select\n",
    "                tokens which are used to compute attention scores\n",
    "                in the self-attention heads, size [bs, seq_length]\n",
    "            head_mask (torch.Tensor): 1.0 in head_mask indicates that\n",
    "                we keep the head, size: [num_heads]\n",
    "                or [num_hidden_layers x num_heads]\n",
    "        Returns:\n",
    "            PyTorch Tensor with predicted class scores\n",
    "        \"\"\"\n",
    "        assert attention_mask is not None, \"attention mask is none\"\n",
    "\n",
    "        # taking BERTModel output\n",
    "        # see https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel\n",
    "        distilbert_output = self.model(\n",
    "            input_ids=features, attention_mask=attention_mask, head_mask=head_mask\n",
    "        )\n",
    "        # we only need the hidden state here and don't need\n",
    "        # transformer output, so index 0\n",
    "        # seq_output = bert_output[0]  # (bs, seq_len, dim)\n",
    "        # mean pooling, i.e. getting average representation of all tokens\n",
    "        # pooled_output = seq_output.mean(axis=1)  # (bs, dim)\n",
    "        # pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        # scores = self.classifier(pooled_output)  # (bs, num_classes)\n",
    "\n",
    "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, num_labels)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=> Loading checkpoint logdir/checkpoints/best.pth\n",
      "loaded state checkpoint logdir/checkpoints/best.pth (global epoch 3, epoch 3, stage train)\n",
      "1/1 * Epoch (test): 100% 220/220 [01:47<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = BertForSequenceClassification(\n",
    "    pretrained_model_name=params[\"model\"][\"model_name\"],\n",
    "    num_classes=params[\"model\"][\"num_classes\"],\n",
    ")\n",
    "runner = SupervisedRunner(input_key=(\"features\", \"attention_mask\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "    runner.infer(\n",
    "        model=model,\n",
    "        loaders=test_loaders,\n",
    "        callbacks=[\n",
    "            CheckpointCallback(\n",
    "                resume=f\"{params['training']['log_dir']}/checkpoints/best.pth\"\n",
    "            ),\n",
    "            InferCallback(),\n",
    "        ],\n",
    "        verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_scores = runner.callbacks[0].predictions[\"logits\"]\n",
    "np.savetxt(X=predicted_scores,\n",
    "           fname=params[\"data\"][\"path_to_test_pred_scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14075"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "len(predicted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_texts['type'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                            sentence source  type\n",
       "0  The son of a Louisiana man whose father was sh...   news  some\n",
       "1  Cameron Sterling, the son of Alton Sterling, w...   news      \n",
       "2  Alton Sterling was killed by Baton Rouge polic...   news      \n",
       "3  Baton Rouge police said in a statement that po...   news      \n",
       "4  The press conference on Wednesday is Cameron's...   news      "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>source</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The son of a Louisiana man whose father was sh...</td>\n      <td>news</td>\n      <td>some</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cameron Sterling, the son of Alton Sterling, w...</td>\n      <td>news</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Alton Sterling was killed by Baton Rouge polic...</td>\n      <td>news</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Baton Rouge police said in a statement that po...</td>\n      <td>news</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The press conference on Wednesday is Cameron's...</td>\n      <td>news</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "combined_texts['type'].iloc[0] = 'some'\n",
    "combined_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs_to_prediction(dataframe, predictions, SENT_TYPES=SENT_TYPES):\n",
    "    from progress.bar import ChargingBar\n",
    "    SENT_TYPES_INVERSE = {i:item for i, item in enumerate(SENT_TYPES)}\n",
    "    if len(dataframe) == len(predictions):\n",
    "        bar = ChargingBar('Predict time', max=len(dataframe))\n",
    "        for i in range(len(dataframe)):\n",
    "            output = predictions[i]\n",
    "            output = int(np.argmax(output))\n",
    "            result = SENT_TYPES_INVERSE[output]\n",
    "            dataframe['type'].iloc[i] = result\n",
    "            bar.next()\n",
    "        dataframe.to_csv('predicted_dataframe.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_to_prediction(combined_texts, predicted_scores)"
   ]
  },
  {
   "source": [
    "# Look at predicted data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = pd.read_csv(\"predicted_dataframe.csv\", encoding='utf-8')\n",
    "predicted_present_simple = pred_data[pred_data['type'] == 'present_simple']\n",
    "predicted_present_simple.to_csv('predicted_present_simple.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "843"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "len(predicted_present_simple)"
   ]
  }
 ]
}