{"title": "How I Built an Intelligent Twitter Bot", "data": [{"type": "subtitle", "content": "Spam on social media bothers me a lot. Here\u2019s how I\u2019m fighting spam on Twitter using machine learning"}, {"type": "image", "content": "https://miro.medium.com/max/13600/1*9s1wWE8H1jSfn-S6n2C49w.png"}, {"type": "sentence", "content": "Twitter users tweet 500 million tweets per day. The volume of information going through Twitter per day makes it one of the best platforms to get information on any subject of interest. In this post, I\u2019ll walk you through how I built a twitter bot with a brain \u2014 powered by machine learning."}, {"type": "sentence", "content": "According to a report, there are 48 million bots on twitter \u2014 and hubofml happened to be one of those bots. Hubofml started as a simple bot written in Node.Js (on one Sunday evening) running on a free Heroku dyno that tracks specific hashtags like machinelearning, computervision, and retweet tweets containing those hashtags."}, {"type": "sentence", "content": "My goal was to use the bot to collate information on machine learning and re-broadcast to people interested in them \u2014 after all, some of the best posts I\u2019ve read on machine learning came from links shared on Twitter by the community."}, {"type": "sentence", "content": "I thought it would be cool to have a bot that tracks hashtags related to machine learning that I follow to stay informed."}, {"type": "sentence", "content": "Days after the bot was deployed, I started noticing some forms of abuse and spam like this:"}, {"type": "sentence", "content": "People would write tweets unrelated to machine learning and hashtag \u201cmachinelearning,\u201d and the bot would retweet it."}, {"type": "sentence", "content": "When we think of spam, it\u2019s easy to see them as unsolicited emails one receives from an unknown person."}, {"type": "sentence", "content": "As odd as it may seem, spam is not limited to email alone these days. Spammers now target everything you can think of. From your inbox to comments on social media posts, you\u2019ll will find spam lurking around."}, {"type": "sentence", "content": "The rest of the post, I will focus on how I use text categorization to combat spam on Twitter."}, {"type": "sentence", "content": "Text categorization is the process of automatically assigning one or more predefined categories to a text document. It has a wide range of use cases like articles categorization, spam detection, intent detection, etc."}, {"type": "sentence", "content": "As I proceed, if you like to take a look at the Jupyter notebook I used for this task, you will find it on Google colab here or in this repository."}, {"type": "subtitle", "content": "Getting Twitter Datasets"}, {"type": "sentence", "content": "There are four ways to obtain Twitter public data. Justin Littman wrote an impressive article on the four approaches."}, {"type": "sentence", "content": "For this task, I used the free (Twitter Standard) APIs. I downloaded about 110k of tweets datasets from Twitter using a python library called Tweepy."}, {"type": "sentence", "content": "I annotated the datasets using two labels: \u201cyes\u201d and \u201cno.\u201d All spam tweets were labelled \"yes\" and non-spam tweets were labelled \"no\"."}, {"type": "sentence", "content": "Spam tweets (in this context) are tweets with no context to the field of machine learning. They are usually about politics, crime, religion, trolling, etc. I retrieved spam tweets based on randomly selected keywords from Wordnet as hashtags."}, {"type": "sentence", "content": "Non-spam (Ham) tweets are tweets related to machine learning, computer vision, NLP, etc. I collected non-spam tweets by tracking hashtags like machinelearning, computervision, NLP on twitter using the Tweepy library."}, {"type": "subtitle", "content": "Data Exploration"}, {"type": "sentence", "content": "My next step was to accrue some knowledge about the data and its nature after downloading the datasets. The datasets contained 99,989 tweets labeled as spam and 10,538 tweets labeled as non-spam."}, {"type": "image", "content": "https://miro.medium.com/max/1692/1*jLl0zqCvD_zZ3MHocuQcVg.png"}, {"type": "sentence", "content": "The data distribution between the two labels shows that the tweet datasets are highly unbalanced. Unbalanced datasets often have substantial effects on how machine learning models generalize. The model could learn that spam tweets are more predominant, making it natural to leans toward the predominant class during generalization."}, {"type": "sentence", "content": "I downsampled the majority class to arrived at an equal number of spam and non-spam (ham) tweets, 10,000 tweets in each class, accounting for a total of 20k tweets in the downsampled dataset."}, {"type": "image", "content": "https://miro.medium.com/max/1436/1*LQpfkjPhxcXmwAre3xPHrg.png"}, {"type": "sentence", "content": "To get further insight into the data, I plotted the top-10 most common words in the two classes. For the non-spam tweets, ai, machinelearning, artificialintellgience, data, data science, biodata, python, and deep learning were found to be most common, as shown in the figure below."}, {"type": "image", "content": "https://miro.medium.com/max/1816/1*LybgoXg-lT5TDlwHr5xZ7w.png"}, {"type": "sentence", "content": "On the other hand, words like good, lol, like, thanks, day, know, and sorry were found to be most common in spam tweets."}, {"type": "image", "content": "https://miro.medium.com/max/2060/1*QAmrM8qwEVQHL7yAV9DP5A.png"}, {"type": "subtitle", "content": "Data Preprocessing"}, {"type": "sentence", "content": "Like text documents, tweets are not exempted from noises. This is, as a result, no formal way of representing tweets. Tweets contain certain special characteristics, such as usernames and retweets, signified by \u201cRT,\u201d links, emoticons, and unimaginable things."}, {"type": "sentence", "content": "It\u2019s essential to clean them up before fitting a model. I applied several preprocessing steps like lowercase conversion, URLs removal, usernames removal, emoticons removal, tokenization, and stemming."}, {"type": "sentence", "content": "In addition, tweets also contain common words that are of little value to the context of the text. These words are known as stop words. Stop words are a set of commonly used words in any language. Hence the removal of stop words from tweets was crucial to focus on the important words in the tweet."}, {"type": "subtitle", "content": "Feature Selections"}, {"type": "subtitle", "content": "Vocabulary list"}, {"type": "sentence", "content": "A vocabulary list is a dictionary of words having each word in the dataset as key and the number of times they occurred as value."}, {"type": "sentence", "content": "A vocabulary list could be:"}, {"type": "sentence", "content": "I built a vocabulary list of all words, excluding stop-words and words with less than 2 occurrences in the datasets."}, {"type": "subtitle", "content": "Tweet Vectorization & Padding"}, {"type": "sentence", "content": "Machine learning models take vectors as input. In order to perform machine learning on text documents, we need to transform text documents into vector representations. This is known as text vectorization."}, {"type": "sentence", "content": "In my approach, I assigned a unique number to each word the vocabularies. Each tweet is encoded using the unique number assigned to the word. If a word could not be found in the dictionary, its automatically assigned a value of 1 \u2014 a value reserved for words that were not found in the vocabulary list."}, {"type": "sentence", "content": "Given the following vocabulary list:"}, {"type": "sentence", "content": "A sentence like \u201cThe cat sat on the mat in the morning\u201d will be encoded as [5,1,3,4,5,2,6,5,0 ]."}, {"type": "sentence", "content": "Finally, I used pad sequence to convert variable length sequence to the same size. This is crucial for the network to take in a batch of variable-length sequences."}, {"type": "subtitle", "content": "Dataset Splitting"}, {"type": "sentence", "content": "I splitted the datasets into training and test set. A training set is used in learning (fitting the model) and test set for testing how the model generalizes on unseen data. 60% of the dataset was used for training, and 40% was used for testing."}, {"type": "subtitle", "content": "Model Architecture"}, {"type": "sentence", "content": "In the past few years, there have been many groundbreaking successes from applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning, etc."}, {"type": "sentence", "content": "RNNs are great, but they suffer from short-term memory. If a tweet is too long, an RNN might have a hard time carrying information from earlier time steps to the current time step."}, {"type": "sentence", "content": "I used an LSTM network, a variant of RNN. LSTM avoids the long-term dependency problem by remembering information for long periods. They have internal features called gates that regulate the flow of information in and out of the cell state. There\u2019s an interesting article on LSTM if you want to know more about how it works."}, {"type": "sentence", "content": "The network architecture consists of three layers, and it\u2019s bidirectional. A bidirectional network allows inputs to be processed from the first to the last and from the last to the first. This ensures that the network is able to preserve information from both past and future."}, {"type": "sentence", "content": "The first layer is the embedding layer, which transforms the input vectors into dense embedding vector."}, {"type": "sentence", "content": "The second layer is the hidden layer, which takes in the dense vector and the previous hidden state to calculate the next hidden state. The final layer takes the final hidden states and feeds it through a fully connected layer, transforming it to the correct output dimension."}, {"type": "sentence", "content": "You can find more on the architecture in the Jupyter notebook."}, {"type": "subtitle", "content": "Deployment & Inferencing"}, {"type": "sentence", "content": "After training the model, I deployed the model artifacts using Flask to Heroku free dyno for real-time inferencing."}, {"type": "sentence", "content": "From tracking tweets to retweeting or liking, the communication process looks like this:"}, {"type": "image", "content": "https://miro.medium.com/max/1542/1*FIFe-ZZ4BwJyENwS3-ZcWg.png"}, {"type": "subtitle", "content": "Conclusion"}, {"type": "sentence", "content": "While the model performed beyond my expectation, however, it\u2019s far from being accurate. Occasionally low-quality tweets still manage to escape the spam filter. I believe that could be improved by using quality datasets."}, {"type": "sentence", "content": "The tweet datasets I used for this task were in tens of thousands; there were tweets with wrong labels. This can be improved by crowdsourcing the labeling task using services like AWS Mechanical Turks."}, {"type": "sentence", "content": "This is my first attempt at combating spam on social media, and I do hope to take this work further in the future."}, {"type": "sentence", "content": "Subscribe here"}], "topic": "social-media"}