{"title": "Building a Synthesizer in TypeScript", "data": [{"type": "subtitle", "content": "How to build a synthesizer in TypeScript with Web Audio API"}, {"type": "subtitle", "content": "Introduction"}, {"type": "sentence", "content": "The TypeScript language provides many features that make it a great choice for audio engineering, which is challenging and requires attention to detail (with clearly audible results when things go wrong)."}, {"type": "sentence", "content": "In this article, we\u2019ll look at how to build a music synthesizer in TypeScript, complete with a keyboard, note readout, oscilloscope, and two oscillators with gain, detune, and delay features, using a few of the many features provided by the Web Audio API for creating and processing sound."}, {"type": "sentence", "content": "For a copy of the example app source code, clone this repo. To see a live demo of the project, check out this page."}, {"type": "subtitle", "content": "Concepts"}, {"type": "sentence", "content": "This project utilizes a few concepts within audio engineering and programming to define the synthesizer in a modular fashion, with each component providing self-contained functionality along with clean interfaces for communicating with other components. These concepts are:"}, {"type": "sentence", "content": "While some understanding of these concepts will be helpful, this article will provide enough guidance for a determined reader to learn the skills used to build this app and others like it."}, {"type": "subtitle", "content": "Main File"}, {"type": "sentence", "content": "The entry point for the application is synth.ts:"}, {"type": "sentence", "content": "The Synth class defines properties for key definitions, the synth engine, and an event bus for each type of event within the synth (signal and control), and HTML elements for the components within the UI. During initialization, the synth queries the document for the required elements, creates instances of the key definition class, synth engine, oscilloscope, and control sliders, and then iterates over the sliders and key definitions to append them to the UI."}, {"type": "sentence", "content": "A listener is attached to the signal bus to pass note on/off events into on_signal which will update the display with the current note."}, {"type": "subtitle", "content": "Event Bus"}, {"type": "sentence", "content": "The EventBus is an implementation of the publish-subscribe pattern, which is ideal for audio components that need to internally manage state independent of other components in the system, each of which are both similar and yet different in varying ways. Let\u2019s check out event-bus.ts:"}, {"type": "sentence", "content": "The event classes extend the base SynthEvent which returns it\u2019s constructor as the type property, creating a very thin implementation of reflection to make it easier for listeners on the bus to switch the event type and handle it accordingly. Each event contains data required by the intended target for the operation being performed. The synth will create two instances of EventBus, one for note on/off events and one for parameter control. UI controls (such as keys and sliders) fire events down the bus, which are received by listeners and forwarded or processed accordingly."}, {"type": "sentence", "content": "This kind of architecture works great for emulating hardware devices which always pass information to one another, and never have any control or even knowledge of the internal workings of other components within the system."}, {"type": "subtitle", "content": "Synth Engine"}, {"type": "sentence", "content": "The next file up for inspection is synth-engine.ts:"}, {"type": "sentence", "content": "SynthEngine defines two local references to the signal and control event bus instances that are passed into the constructor, along with an audio context and gain, oscillator, and delay components, which we will examine shortly. The constructor also connects each oscillator to the delay of the same channel, connects each delay to the master output gain, and then listens for events using separate handlers for SynthEvent and ControlEvent."}, {"type": "sentence", "content": "When the engine receives a note on/off event, the event is forwarded to each of the two oscillators to trigger a start/stop operation. Similarly, when a control event is received, a property is set on the component with matching type and channel. Each property handles the provided value internally and will perform necessary math and other operations within, to allow it full control of itself while providing a clean, simple interface."}, {"type": "subtitle", "content": "Synth Keys"}, {"type": "sentence", "content": "The SynthKey and KeyGenerator classes are defined in synth-keys.ts:"}, {"type": "sentence", "content": "KeyDefinition defines one octave of notes along with two octaves in a scale which will be used by the keys property to generate an array of HTML buttons. When SynthKey.create is called for each key definition, a button is created and arrow (lambda) functions are assigned into the button event handlers to fire note on/off events down the bus."}, {"type": "subtitle", "content": "Slider"}, {"type": "sentence", "content": "The slider widget class is located in slider.ts:"}, {"type": "sentence", "content": "The slider is simple in design and returns a range input element to be placed on the synth parameter control panel, with min/max/default values and an event handler on the input that will fire an event down the event bus with channel, control type, and updated value."}, {"type": "subtitle", "content": "Synth Component Base"}, {"type": "sentence", "content": "Let\u2019s check out the component base class in synth-component.ts:"}, {"type": "sentence", "content": "This simple abstract class defines a typed property for node that can be any of the Web Audio API objects which have been wrapped in a class that extends this one. Each component will inherit properties for audio context, channel, and the component to receive a signal from this one\u2019s output."}, {"type": "sentence", "content": "Also inherited is the drive method, which is used to forward audio signal data to the target component in the signal chain. This allows each component to restart and reconnect to the target automatically as required, and to be passed through batch operations with simplicity."}, {"type": "sentence", "content": "This takes full advantage of both functional and object-oriented paradigms, allowing the synth to be expanded and upgraded while remaining in perfect working order. This architecture is analogous to real audio gear, with standardized IO connections for both audio and control data."}, {"type": "subtitle", "content": "Gain"}, {"type": "sentence", "content": "The simplest audio component is Gain, defined in gain.ts:"}, {"type": "sentence", "content": "Gain creates a GainNode to be used internally, and simply passes the gain property of that object through the gain property. Objects handling this one can therefore modify the gain of the node without access to other properties on the GainNode instance. If the channel provided to the constructor is CHANNEL.MASTER the component will connect to the audio context\u2019s destination (your device\u2019s audio driver) and set the gain to 10%, which is important because the synthesizer is very loud by default. This is an important concept in audio engineering as it isn\u2019t hard to blow speakers and damage eardrums, both of which are very mechanically sensitive."}, {"type": "subtitle", "content": "Oscillator"}, {"type": "sentence", "content": "Next, let\u2019s check out the oscillator component in oscillator.ts:"}, {"type": "sentence", "content": "The oscillator holds a reference to a Web Audio OscillatorNode, which is re-created every time a note is played because once the internal oscillator is stopped it cannot be restarted. The waveform can be either sine or square, and two internal gain nodes are created, one to serve as a controllable volume level, and the other as a volume envelope to quickly ramp up the volume when a note starts and back down when it stops (this removes a \u2018pop\u2019 sound that is audible when the internal oscillator starts or stops, which is loud and could damage equipment at high volumes)."}, {"type": "sentence", "content": "The gain and detune properties allow these parameters to be set externally by the engine when processing a control event, and each perform the math required to transform the incoming raw slider value into a value that is compatible with the target audio node."}, {"type": "sentence", "content": "When play is called, a fresh oscillator node is created and configured with frequency, detune, and waveform, then the internal audio chain is established and the oscillator is started and ramped-up in volume."}, {"type": "subtitle", "content": "Delay"}, {"type": "sentence", "content": "The delay component is defined in delay.ts:"}, {"type": "sentence", "content": "Delay contains a node property which it will use to receive a signal from other components, along with a Web Audio DelayNode and two additional gain nodes for dry/wet mixing. When a Delay component is created, the internal audio chain is set up and a delay value of 0.2 seconds is set."}, {"type": "subtitle", "content": "Oscilloscope"}, {"type": "sentence", "content": "The final output waveform of the synth is rendered by scope.ts:"}, {"type": "sentence", "content": "At the core of the oscilloscope is the web audio AnalyserNode, which provides real-time frequency and time-domain analysis for any audio signal it receives as input, along with the HTML Canvas, which provides 2D graphics features that are perfect for rendering an audio waveform. An AnalyserNode is created with an FFT size of 2048, canvas rendering properties are set, and the scope is started with run which will retrieve data from the analyzer and render it."}, {"type": "sentence", "content": "Render cycles are queued up with requestAnimationFrame which will run the cycle the next time the browser engine is ready to repaint the screen. The waveform is retrieved by stepping over the audio samples and tracking the current phase, returning the captured waveform when the phase has reached 360 degrees. The captured waveform is then rendered by calculating the horizontal step size and iterating over the width of the canvas, drawing a point along the y axis corresponding to the amplitude of the sample at each step. Dashed lines are also added along the x/y axes for reference."}, {"type": "subtitle", "content": "Conclusion"}, {"type": "image", "content": "https://miro.medium.com/max/1280/1*JT5VcoSV7gPpQoHxwFq2Tw.gif"}, {"type": "sentence", "content": "TypeScript provides excellent features for implementing high-performance applications with architecture suited for work in audio engineering, graphics, and other asynchronous programs where small performance bugs are audible and/or visible to the user."}, {"type": "sentence", "content": "This synth illustrates just a few of the components provided by the Web Audio API for generating, processing, and playing sounds of all kinds."}, {"type": "sentence", "content": "Thanks for reading!"}, {"type": "sentence", "content": "~ 8_bit_hacker"}], "topic": "ux"}