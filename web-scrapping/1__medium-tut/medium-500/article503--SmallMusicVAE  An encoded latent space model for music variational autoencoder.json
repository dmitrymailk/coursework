{"title": "SmallMusicVAE: An encoded latent space model for music variational autoencoder", "data": [{"type": "sentence", "content": "Google Magenta is an awesome open source project focusing on creative process being done by machine learning. What attracts me the most is its ability to generate novel melodies with coherence and natural sounds, especially MusicVAE project and MidiMe project. In this post, I will try to briefly explain the model architecture of both projects as well as how to generate catchy melody with my MidiMe Python implementation."}, {"type": "subtitle", "content": "What is MusicVAE?"}, {"type": "sentence", "content": "MusicVAE is a Variational Autoencoder model on music sequences, if you want to know more about mathematical equations behind this model, I think this post is the best in class for this purpose. Here, I will try to show you the overall architecture as well as the core ideas to implement this model. Below is a diagram that illustrates my understanding about this model:"}, {"type": "sentence", "content": "MusicVAE is basically a RNN model with encoder part (blue blocks) and decoder part (red blocks). Since this is a RNN model, the encoder and decoder parts can be anything within this RNN family, ranging from simple RNN cells, GRU cells or LSTM cells. Here in MusicVAE model, it will be bidirectional LSTM cells (for simple illustration, the picture does not show this)."}, {"type": "sentence", "content": "The magic behind the Variation Autoencoder part lies in the way we learn the Z distribution of the model (the circle one). The Z distribution can be learned from the output of the affine layers with previous final encoder states to get the mu and sigma vectors, which in turn will be used to generate Z distribution by using Multivariate Normal distribution with mean equals to mu vectors and the diagonal covariance matrix will be represented by sigma vectors."}, {"type": "sentence", "content": "The loss function of this model consists of 2 parts:"}, {"type": "sentence", "content": "With all this in mind, we can start implementing and training the model. Next comes a melody generation part. After learning the Z distribution and other model parameters, we can generate melody snippets by randomly sampling from N(0, 1) and using this as initial stages for decoder parts to generate new melodies. This process can be illustrated in the picture below:"}, {"type": "image", "content": "https://miro.medium.com/max/1654/1*mQCSvxtZ7DNRukFU-7M9UQ.png"}, {"type": "sentence", "content": "This process is made possible due to the fact that we incorporate KL divergence into our loss function. Thus implicitly telling the model to learn the Z distribution as N(0, 1). To hear some of the melody samples generating from this model, you can visit the official MusicVAE page for that."}, {"type": "subtitle", "content": "MidiMe architecture"}, {"type": "sentence", "content": "The purpose of MusicVAE is to find the desirable latent space that has 3 following properties (quote from the official MusicVAE):"}, {"type": "sentence", "content": "Thus, what MusicVAE model trying to find is a compact latent distribution Z that incapsulates all music melodies which sound natural and coherence to human ears. But what if you want this latent to be even more compact, you desire it to be even smaller and only conveys a small catchy melody snippet which you really like. In other words, you wish to find your own personalized latent space which contains all melodies that sound like your original melody! This is where MidiMe idea comes into play."}, {"type": "sentence", "content": "The core idea of MidiMe is elegant, we just try to find a smaller latent vector space by repeatedly training on the melody snippet we want to learn. It looks something like this:"}, {"type": "image", "content": "https://miro.medium.com/max/1852/1*BhHZQ_v1CgT3dG3hlUj1Hw.png"}, {"type": "sentence", "content": "As shown in the diagram, we already had the distribution Z (by loading from pre-trained MusicVAE model), what we want to learn is the distribution z\u2019. We can apply the same intuition from MusicVAE to learn this smaller latent space, but instead of using RNN model, the encoder and decoder parts are just fully connected layers. We still use KL divergence in here but use mean square error loss for output part instead of cross entropy from MusicVAE."}, {"type": "sentence", "content": "Generating melody samples with MidiMe is a little bit different from MusicVAE, here we will sample from smaller z\u2019 distribution and pass this through autoencoder decoder part to generate RNN decoder initial stages. The rest are the same with MusicVAE:"}, {"type": "image", "content": "https://miro.medium.com/max/1892/1*xHVCS2UYH4bgXr_d6B99eg.png"}, {"type": "sentence", "content": "You can play around with this model by visiting this cool Magenta MidiMe website. Since this model is originally implemented in Javascript, I saw it as an opportunity to reimplement this model in my beloved Python language, and named it SmallMusicVAE! Here is the repository of my SmallMusicVAE model implementation."}, {"type": "sentence", "content": "Here are some of the sample melodies generated from my SmallMusicVAE that I would like to share. For the first example, I used Beethoven Fur Elise as the melody I would like my model to learn:"}, {"type": "sentence", "content": "Here is the melody sample generated from my model:"}, {"type": "sentence", "content": "As you can see, the model can capture the overall melody and produce something similar to that. It still retains the original motifs as well as music patterns from original melody."}, {"type": "sentence", "content": "As for the second example, I tried something harder with more notes and complex patterns. This is a monophonic Chopin \u2014 Fantaisie-Impromptu (Op. 66):"}, {"type": "sentence", "content": "And here is the melody sample generated from my model:"}, {"type": "sentence", "content": "The latent space still can capture the fast pace tempo as well as the main music pattern."}, {"type": "sentence", "content": "All the samples above can be trained on the local machine since we only try to obtain a much smaller latent vector space, thus dramatically reduces the number of parameters in the model. To achieve those results, it only takes me 200 training steps which takes around 5\u201310 minutes!"}, {"type": "subtitle", "content": "LCMusicVAE a Latent Contraint VAE (Experimentation)"}, {"type": "sentence", "content": "Fascinating with the idea of latent vector space for music generation, I also tried to implement the model from Latent Translation: Crossing Modalities by Bridging Generative Models. If you have read the paper, you will notice that MidiMe is actually a special case of this paper model. But in MidiMe model, it does not have the cross entropy loss with real output event (it only has MSE loss with Z sampling output and KL divergence)."}, {"type": "sentence", "content": "Thus for this LCMusicVAE model, I incorporated the bidirectional LSTM model to act as decoder part so that I can output predictions on target music events. By adding the ability to output predictions on music events, I can add the cross entropy loss to the model, so the loss function of this model will be determined by 3 parts: KL divergence, MSE loss and cross entropy loss. As for Sliced Wasserstein Distance loss from the paper, I did not use this because my model only learns 1 latent space instead of 2 as in the paper."}, {"type": "sentence", "content": "The sample results from this model is not as good as my expectation, it generates very random melody (or not even a melody) and not similar to the original at all. I suspect this bad result is attributed to 3 main things:"}, {"type": "sentence", "content": "There are so many things I can try to make this model results better, so I also included this LCMusicVAE model into my SmallMusicVAE repo, for those who want to improve or try this out."}, {"type": "subtitle", "content": "Closing thoughts"}, {"type": "sentence", "content": "I hope you enjoy reading this as much as I implement SmallMusicVAE project and write this post! For the upcoming article, I will try to create the whole music generation pipeline, from generating initial melody with SmallMusicVAE to polyphonic music patterns with Music Transformer. Stay tuned!"}, {"type": "subtitle", "content": "References"}, {"type": "sentence", "content": "Latent Translation: Crossing Modalities by Bridging Generative Models"}], "topic": "artificial-intelligence"}