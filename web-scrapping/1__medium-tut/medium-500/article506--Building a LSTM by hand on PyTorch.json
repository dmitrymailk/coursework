{"title": "Building a LSTM by hand on PyTorch", "data": [{"type": "subtitle", "content": "Being able to build a LSTM cell from scratch enable you to make your own changes on the architecture and takes your studies to the next level."}, {"type": "image", "content": "https://miro.medium.com/max/900/1*s7_EO0rjXAw99RnH1x4s_g.png"}, {"type": "sentence", "content": "All the code mentioned are on the gists below or in our repo."}, {"type": "sentence", "content": "The LSTM cell is one of the most interesting architecture on the Recurrent Neural Networks study field on Deep Learning: Not only it enables the model to learn from long sequences, but it also creates a numerical abstraction for long and short term memories, being able o substitute one for another whenever needed."}, {"type": "sentence", "content": "On this post, not only we will be going through the architecture of a LSTM cell, but also implementing it by-hand on PyTorch."}, {"type": "sentence", "content": "Last but not least, we will show how to do minor tweaks on our implementation to implement some new ideas that do appear on the LSTM study-field, as the peephole connections."}, {"type": "subtitle", "content": "The LSTM Architecture"}, {"type": "sentence", "content": "The LSTM has we is called a gated structure: a combination of some mathematical operations that make the information flow or be retained from that point on the computational graph. Because of that, it is able to \u201cdecide\u201d between its long and short-term memory and output reliable predictions on sequence data:"}, {"type": "image", "content": "https://miro.medium.com/max/4000/0*VXNy36Ay_Rq3m1LE.png"}, {"type": "sentence", "content": "We will go from it part-by-part:"}, {"type": "subtitle", "content": "The forget gate"}, {"type": "sentence", "content": "The forget gate is the one where the input information is operated along with the candidate, as the long term memory. See that, on the first linear combination of the input, hidden state and bias, it is applied a sigmoid function:"}, {"type": "image", "content": "https://miro.medium.com/max/4000/0*huVOAJFuhX3bSWH5.png"}, {"type": "sentence", "content": "That sigmoid \u201cscales\u201d the output of the forget gate from 0 to 1 \u2014 and, by multiplying it with the candidate, we can either set it to zero, which represent a \u201cforgetting\u201d from the long time memory, or by a bigger number, which represent \u201chow much\u201d we are remembering from that long-term memory."}, {"type": "subtitle", "content": "The input gate and solution of the new long-term memory"}, {"type": "sentence", "content": "The input gate is where the information contained on the input and hidden state is combined and then operated along with the candidate and partial candidate c\u2019_t:"}, {"type": "image", "content": "https://miro.medium.com/max/4000/0*lipRMhGsnzGrEhaS.png"}, {"type": "sentence", "content": "On those operations, it is decided how much of the new information will be introduced on the memory how it will change \u2014 that\u2019s why we use a tanh function (\u201cscale\u201d from -1 to 1). We combine the partial candidate from the short-term and long-term memories and set it as the candidate."}, {"type": "sentence", "content": "And now we can proceed to the output gate."}, {"type": "subtitle", "content": "The output gate and hidden state (output) of the cell"}, {"type": "sentence", "content": "After that, we can gather o_t as the output gate of the LSTM cell and then multiply it per the tanh of the candidate (long-term memory) which was already update with the proper operation. The output of network will be h_t."}, {"type": "image", "content": "https://miro.medium.com/max/4000/0*lirs3HLo70pNlj0O.png"}, {"type": "sentence", "content": "At the end, we have:"}, {"type": "subtitle", "content": "Equations of the LSTM cell:"}, {"type": "image", "content": "https://miro.medium.com/max/832/1*cmpPLozIVuuZQ2EauZIZFg.png"}, {"type": "subtitle", "content": "Implementing it on PyTorch"}, {"type": "sentence", "content": "To implement it on PyTorch, we will first do the proper imports."}, {"type": "sentence", "content": "We will now create its class by inheriting from nn.Module , and then also instance its parameters and weight initialization, which you will see below (notice that its shapes are decided by the input size and output size of the network):"}, {"type": "subtitle", "content": "Setting the parameters"}, {"type": "sentence", "content": "To understand the shape of each of the weights, let\u2019s see:"}, {"type": "sentence", "content": "The input shape of the matrix is (batch_size, sequence_length, feature_length) \u2014 and so the weight matrix that will multiply each element of the sequence must have the shape (feature_length, output_length)."}, {"type": "sentence", "content": "The hidden state (a.k.a. output), for each of element on the sequence has the shape (batch_size, output_size), which results, at the end of the sequence processing, an output shape of (batch_size, sequence_length, output_size). \u2014 Because of that, the weight_matrix that will multiply it must have the shape (output_size, output_size) which corresponds to the parameter hidden_sz of our cell."}, {"type": "sentence", "content": "And here is the weight initialization, which we use as the same as the one in PyTorch default nn.Module s:"}, {"type": "subtitle", "content": "Feedforward operation"}, {"type": "sentence", "content": "The feedforward operation receives the init_states parameter, which is a tuple with the (h_t, c_t) parameters of the equations above, which is set to zero if not introduced. We then perform the feedforward of the LSTM equations for each of the sequence elements preserving the (h_t, c_t), and introducing it as the states for the next element of the sequence."}, {"type": "sentence", "content": "At the end, we return the predictions and the last states tuple. Let\u2019s see how it happens:"}, {"type": "subtitle", "content": "Now and optimized version"}, {"type": "sentence", "content": "This LSTM is correct in terms of operations but not very optimized in terms of computational time: we perform 8 matrix multiplications separately, which is much slower than doing it in a vectorized way. We will now show how it could be done by reducing it to 2 matrix multiplications, which would make it much faster."}, {"type": "sentence", "content": "To do this operations, we set two matrixes, U and V, which have the weight contained on the 4 matrix multiplications each of they do. We then perform the gated operations on the matrixes that already passed per the linear combinations + bias operation."}, {"type": "sentence", "content": "With the vectorized operations, the equations of the LSTM cell would be:"}, {"type": "image", "content": "https://miro.medium.com/max/1276/1*glHcEaVYSqhSgonWQ4zFcQ.png"}, {"type": "sentence", "content": "And so it\u2019s nn.Module class would be:"}, {"type": "subtitle", "content": "Optimized LSTM cell class"}, {"type": "sentence", "content": "Last but not least, we can show how easy would it be to tweak your implementation to use LSTM peephole connections."}, {"type": "subtitle", "content": "LSTM peephole"}, {"type": "sentence", "content": "The LSTM peephole has minor tweaks on its feedforward operation that change it, the optimized case to:"}, {"type": "image", "content": "https://miro.medium.com/max/1280/1*83ZFW20BItVrG4-Ghhiceg.png"}, {"type": "sentence", "content": "It occurs that, by having a well implemented and optimized implementation of LSTM, we can add the options for peephole connection with some minor tweak on it:"}, {"type": "sentence", "content": "And with that our LSTM is done. You may want to see it on our repo and test it with our LSTM text-sentiment analysis notebook which we made ready for testing and comparing with torch LSTM built-in layer."}, {"type": "subtitle", "content": "Conclusion"}, {"type": "sentence", "content": "We can take as a conclusion that, despite being kind of a Deep Learning tabus, if done in steps and with clean and good coding, it is actually easy to perform it\u2019s operations into a clean, easy to use nn.Module . We\u2019ve also seen how easy would it be to change and tweak its connections to do operations like the peephole connection."}, {"type": "subtitle", "content": "References"}], "topic": "artificial-intelligence"}