{"title": "Parameters Optimization Explained", "data": [{"type": "subtitle", "content": "A brief yet descriptive guide to Gradient Descent, ADAM, ADAGRAD, RMSProp, NADAM"}, {"type": "image", "content": "https://miro.medium.com/max/3840/1*qebmIPhmI8ZBfVgkCjkHRw.jpeg"}, {"type": "sentence", "content": "Optimization refers to the task of minimizing/maximizing an objective function f(x) parameterized by x. In machine/deep learning terminology, it\u2019s the task of minimizing the cost/loss function J(w) parameterized by the model\u2019s parameters w \u2208 R^d.Optimization algorithms (in the case of minimization) have one of the following goals:"}, {"type": "subtitle", "content": "Gradient Descent"}, {"type": "sentence", "content": "Gradient Descent is an optimizing algorithm used in Machine/ Deep Learning algorithms. The goal of Gradient Descent is to minimize the objective convex function f(x) using iteration."}, {"type": "image", "content": "https://miro.medium.com/max/2800/0*sZQe_pi5y3syluwH.jpeg"}, {"type": "sentence", "content": "Let\u2019s see how Gradient Descent actually works by implementing it on a cost function."}, {"type": "image", "content": "https://miro.medium.com/max/2800/0*U8nBDCjRiGO63rXg.jpeg"}, {"type": "sentence", "content": "For ease, let\u2019s take a simple linear model."}, {"type": "sentence", "content": "A machine learning model always wants low error with maximum accuracy, in order to decrease error we will intuit our algorithm that you\u2019re doing something wrong that is needed to be rectified, that would be done through Gradient Descent."}, {"type": "sentence", "content": "We need to minimize our error, in order to get a pointer to minima, we need to walk some steps that are known as alpha(learning rate)."}, {"type": "subtitle", "content": "Steps to implement Gradient Descent"}, {"type": "image", "content": "https://miro.medium.com/max/836/0*I_XyWM7SP2A73LUZ.png"}, {"type": "sentence", "content": "3. Repeat until slope =0"}, {"type": "sentence", "content": "A derivative is a term that comes from calculus and is calculated as the slope of the graph at a particular point. The slope is described by drawing a tangent line to the graph at the point. So, if we are able to compute this tangent line, we might be able to compute the desired direction to reach the minima."}, {"type": "sentence", "content": "Learning rate must be chosen wisely as:1. if it is too small, then the model will take some time to learn.2. if it is too large, model will converge as our pointer will shoot and we\u2019ll not be able to get to minima."}, {"type": "image", "content": "https://miro.medium.com/max/3928/1*8lwt2yBmLqlB0VCbweDvSQ.jpeg"}, {"type": "sentence", "content": "Vanilla gradient descent, however, does not guarantee good convergence, but offers a few challenges that need to be addressed:"}, {"type": "image", "content": "https://miro.medium.com/max/3952/1*RV_fZzePemKdIA4SzTr-Ng.jpeg"}, {"type": "sentence", "content": "In simple words, every step we take towards minima tends to decrease our slope, now if we visualize, in steep region of curve derivative is going to be large therefore steps taken by our model too would be large but as we will enter gentle region of slope our derivative will decrease and so will the time to reach minima."}, {"type": "sentence", "content": "To know more about Gradient Descent methods and it\u2019s strategies follow: -"}, {"type": "subtitle", "content": "ADAGRAD(Adaptive Gradient Algorithm)"}, {"type": "sentence", "content": "The drawback of Gradient Descent was decay of learning rate due to which our pointer wasn\u2019t able to converge. Now to overcome this drawback we intuit to change learning rate for each and every layer, each and every iteration and each and every neuron as well."}, {"type": "sentence", "content": "Adagrad eliminates the need to manually tune the learning rate."}, {"type": "sentence", "content": "The reason to adaptively change learning rate is because there are two types of data :-"}, {"type": "sentence", "content": "In case of sparse data, we would experience sparse ON(1) features and more frequent OFF(0) features, now, most of the time gradient update will be NULL as derivative is zero in most cases and when it will be one, the steps would be too small to reach minima."}, {"type": "sentence", "content": "For frequent features we require low learning rate, but for high features we require high learning rate."}, {"type": "sentence", "content": "So, in order to boost our model for sparse nature data, we need to chose adaptive learning rate."}, {"type": "image", "content": "https://miro.medium.com/max/2984/1*BbhG9QdPpilcOpjnIx9IZQ.jpeg"}, {"type": "sentence", "content": "G(t) is the current step and G(t-1) is the previous step, Epsilon is included as if G(t) becomes zero our learning rate will crash so in order to maintain consistency we use epsilon which denotes a random small positive number."}, {"type": "sentence", "content": "In the denominator, we accumulate the sum of the square of the past gradients. Each term is a positive term so it keeps on growing to make the learning rate \u03b7 infinitesimally small to the point that algorithm is no longer able learning."}, {"type": "sentence", "content": "If you\u2019re familiar with working of Gradient Descent, you must\u2019ve noticed with every step taken towards minima our learning rate shortens which leads to its decay and more time consumption, that\u2019s fixed using ADAGRAD."}, {"type": "sentence", "content": "Advantages:-"}, {"type": "sentence", "content": "Parameters corresponding to sparse features gets better updates."}, {"type": "sentence", "content": "Disadvantages:-"}, {"type": "sentence", "content": "Learning rate decays very aggressively as denominator grows."}, {"type": "subtitle", "content": "RMSProp"}, {"type": "sentence", "content": "The main aim for RMSProp is to prevent overly aggressive behavior of G(t)in ADAGRAD. Now if we go deep in ADAGRAD we can comprehend that G(t) is exploding as we are collectively aggregating square of past gradient from first iteration to last iteration."}, {"type": "sentence", "content": "To escape blasting of G(t) we intend to average the decay in order to make process more smooth and less time consuming."}, {"type": "image", "content": "https://miro.medium.com/max/2560/1*7jMhx7O1cE_hB2zBny-5GQ.jpeg"}, {"type": "sentence", "content": "Even if our G(t) increases, it will be diminished to small quantity as we are computing its product by (1-\u04112) which is itself very small thus cancelling its high effect."}, {"type": "sentence", "content": "Here, the learning rate is decreasing but not as fast as in case of ADAGRAD thus making convergence more fluid with less time consumption."}, {"type": "sentence", "content": "In nutshell, Rmsprop is a very clever way to deal with the problem. It uses a moving average of squared gradients to normalize the gradient itself. That has an effect of balancing the step size, decrease the step for large gradient to avoid exploding, and increase the step for small gradient to avoid vanishing."}, {"type": "subtitle", "content": "ADAM"}, {"type": "sentence", "content": "The algorithms leverages the power of adaptive learning rates methods to find individual learning rates for each parameter. It also has advantages of Adagrad, which works really well in settings with sparse gradients, but struggles in non-convex optimization of neural networks, and RMSprop, which tackles to resolve some of the problems of Adagrad and works really well in on-line settings."}, {"type": "sentence", "content": "Adam can be looked at as a combination of RMSprop and Stochastic Gradient Descent with momentum. It uses the squared gradients to scale the learning rate like RMSprop and it takes advantage of momentum by using moving average of the gradient instead of gradient itself like SGD with momentum."}, {"type": "image", "content": "https://miro.medium.com/max/2320/1*0l1SOnXJA4I93J5t7_NTfA.jpeg"}, {"type": "sentence", "content": "Adam algorithm first updates the exponential moving averages of the gradient(mt) and the squared gradient(vt) which is the estimates of the first and second moment."}, {"type": "sentence", "content": "Hyper-parameters \u03b21, \u03b22 \u2208 [0, 1) control the exponential decay rates of these moving averages as shown below"}, {"type": "image", "content": "https://miro.medium.com/max/2364/0*6exMoSzi5HVJDm19.png"}, {"type": "sentence", "content": "Moving averages are initialized as 0 leading to moment estimates that are biased around 0 especially during the initial timesteps. This initialization bias can be easily counteracted resulting in bias-corrected estimates"}, {"type": "image", "content": "https://miro.medium.com/max/2800/0*qX3lb8herTGJxriM.png"}, {"type": "sentence", "content": "Finally, we update the parameter as shown below"}, {"type": "image", "content": "https://miro.medium.com/max/920/0*skOYntaau1Cyz2-P.png"}, {"type": "sentence", "content": "Adam implements the exponential moving average of the gradients to scale the learning rate instead of a simple average as in Adagrad. It keeps an exponentially decaying average of past gradients."}, {"type": "sentence", "content": "Adam is computationally efficient and has very little memory requirement."}, {"type": "subtitle", "content": "NADAM(Nesterov-accelerated Adaptive Moment Estimation)"}, {"type": "image", "content": "https://miro.medium.com/max/1240/0*V8zv4tCWPBdfCyCQ.gif"}, {"type": "subtitle", "content": "Conclusion"}, {"type": "sentence", "content": "Hopefully, this article has increased your understanding of optimization techniques."}, {"type": "sentence", "content": "As always, thank you so much for reading, and please share this article if you found it useful! :)"}, {"type": "subtitle", "content": "References:"}, {"type": "sentence", "content": "[1] An overview of gradient descent optimization algorithms By Sebastian Ruder"}, {"type": "sentence", "content": "[2] An Empirical Comparison of Optimizers for Machine Learning Models By Rick Wierenga"}, {"type": "sentence", "content": "[3] Overview of different Optimizers for neural networks By Renu Khandelwal"}, {"type": "sentence", "content": "[4] Adam \u2014 latest trends in deep learning optimization By Vitaly Bushaev"}, {"type": "sentence", "content": "[5] Learning Parameters, Part 5: AdaGrad, RMSProp, and Adam By Akshay L Chandra"}, {"type": "sentence", "content": "The cover template is designed by me on canva.com, the source is mentioned on every visual, and the un-mentioned visuals are from my notebook or are designed by me on Paint 3D."}, {"type": "sentence", "content": "Feel free to connect:"}, {"type": "sentence", "content": "Check my other articles:-"}, {"type": "sentence", "content": "Follow for further Machine Learning/ Deep Learning blogs."}], "topic": "artificial-intelligence"}