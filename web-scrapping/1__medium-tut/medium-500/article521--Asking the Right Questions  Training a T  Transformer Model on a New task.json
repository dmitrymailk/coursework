{"title": "Asking the Right Questions: Training a T5 Transformer Model on a New task", "data": [{"type": "subtitle", "content": "The T5 Transformer frames any NLP task as a text-to-text task enabling pre-trained models to easily learn new tasks. Let\u2019s teach the old dog a new trick!"}, {"type": "sentence", "content": "I\u2019ve been itching to try the T5 (Text-To-Text Transfer Transformer) ever since it came out way, way back in October 2019 (it\u2019s been a long couple of months). I messed around with open-sourced code from Google a couple of times, but I never managed to get it to work properly. Some of it went a little over my head (Tensorflow \ud83d\ude2b ) so I figured I\u2019ll wait for Hugging Face to ride to the rescue! As always, the Transformers implementation is much easier to work with and I adapted it for use with Simple Transformers."}, {"type": "sentence", "content": "Before we get to the good stuff, a quick word on what the T5 model is and why it\u2019s so exciting. According to the article on T5 in the Google AI Blog, the model is a result of a large-scale study (paper link) on transfer learning techniques to see which works best. The T5 model was pre-trained on C4 (Colossal Clean Crawled Corpus), a new, absolutely massive dataset, released along with the model."}, {"type": "sentence", "content": "Pre-training is the first step of transfer learning in which a model is trained on a self-supervised task on huge amounts of unlabeled text data. After this, the model is fine-tuned (trained) on smaller labelled datasets tailored to specific tasks, yielding far superior performance compared to simply training on the small, labelled datasets without pre-training. Further information on pre-training language models can be found in my post below."}, {"type": "sentence", "content": "A key difference in the T5 model is that all NLP tasks are presented in a text-to-text format. On the other hand, BERT-like models take a text sequence as an input and output a single class label or a span of text from the input. A BERT model is retrofitted for a particular task by adding a relevant output layer on top of the transformer model. For example, a simple linear classification layer is added for classification tasks. T5, however, eschews this approach and instead reframes any NLP task such that both the input and the output are text sequences. This means that the same T5 model can be used for any NLP task, without any aftermarket changes to the architecture. The task to be performed can be specified via a simple prefix (again a text sequence) prepended to the input as demonstrated below."}, {"type": "image", "content": "https://miro.medium.com/max/1280/0*V9hMIf-iNtfrUbPD.gif"}, {"type": "sentence", "content": "The T5 paper explores many of the recent developments in NLP transfer learning. It is well worth a read!"}, {"type": "sentence", "content": "However, the focus of this article on adapting the T5 model to perform new NLP tasks. Thanks to the unified text-to-text approach, this turns out to be (surprisingly) easy. So, let\u2019s get to the aforementioned good stuff!"}, {"type": "subtitle", "content": "The Task"}, {"type": "sentence", "content": "The T5 model is trained on a wide variety of NLP tasks including text classification, question answering, machine translation, and abstractive summarization. The task we will be teaching our T5 model is question generation."}, {"type": "sentence", "content": "Specifically, the model will be tasked with asking relevant questions when given a context."}, {"type": "sentence", "content": "You can find all the scripts used in this guide in the examples directory of the Simple Transformers repo."}, {"type": "subtitle", "content": "The Dataset"}, {"type": "sentence", "content": "We will be using the Amazon Review Data (2018) dataset which contains (among other things) descriptions of the various products on Amazon and question-answer pairs related to those products."}, {"type": "sentence", "content": "The descriptions and the question-answer pairs must be downloaded separately. You can either download the data manually by following the instructions in the Descriptions and Question-Answer Pairs below, or you can use the provided shell script. The list of categories used in this study is given below."}, {"type": "subtitle", "content": "Descriptions"}, {"type": "subtitle", "content": "Question-Answer Pairs"}, {"type": "subtitle", "content": "Shell Script"}, {"type": "sentence", "content": "Alternatively, the shell script below should download all the necessary files by reading the links from the two text files also given below (place the text files in the same directory data/ as the shell script). It will also rename meta_ALL_Beauty.json.gz to meta_Beauty.json.gz to match the name in the question-answer file."}, {"type": "sentence", "content": "With the data files in place, we can start training our model!"}, {"type": "subtitle", "content": "Setup"}, {"type": "sentence", "content": "We will be using the Simple Transformers library (based on the Hugging Face Transformers) to train the T5 model."}, {"type": "sentence", "content": "The instructions given below will install all the requirements."}, {"type": "sentence", "content": "See installation docs"}, {"type": "subtitle", "content": "Data Preparation"}, {"type": "sentence", "content": "We can process the data files and save them in a convenient format using the script given below. This will also split the data into train and evaluation sets."}, {"type": "sentence", "content": "Adapted from the helpful scripts given in the Amazon Review Data page."}, {"type": "sentence", "content": "Check whether you have the train_df.tsv and eval_df.tsv files in your data/ directory."}, {"type": "subtitle", "content": "Training the Model"}, {"type": "subtitle", "content": "Data Formats"}, {"type": "sentence", "content": "The input data to a T5 model should be a Pandas DataFrame containing 3 columns as shown below."}, {"type": "sentence", "content": "Internally, Simple Transformers will build the properly formatted input and target sequences (shown below) from the Pandas DataFrame."}, {"type": "sentence", "content": "The input to a T5 model has the following pattern;"}, {"type": "sentence", "content": "The target sequence has the following pattern;"}, {"type": "sentence", "content": "The prefix value specifies the task we want the T5 model to perform. To train a T5 model to perform a new task, we simply train the model while specifying an appropriate prefix. In this case, we will be using the prefix ask_question. I.e. All the rows in our DataFrame will have the value ask_question in the prefix column."}, {"type": "subtitle", "content": "Training"}, {"type": "sentence", "content": "Training the model is quite straightforward with Simple Transformers."}, {"type": "sentence", "content": "As you might observe from the training script, we are using the t5-large pre-trained model. Using these parameters with the t5-large model takes about 12 hours of training with a single Titan RTX GPU. Depending on your GPU resources, you can either increase the train_batch_size to speed up training or you can decrease it to fit a GPU with less VRAM (Titan RTX has 24 GB)."}, {"type": "sentence", "content": "Note that you can offset the effect of a small batch size by increasing the gradient_accumulation_steps. The effective batch size is roughly equal to train_batch_size * gradient_accumulation_steps."}, {"type": "sentence", "content": "You can also significantly improve the training speed and GPU memory consumption by opting for the t5-base model. This will likely result in comparatively worse (but by no means poor) model."}, {"type": "sentence", "content": "This training script will also automatically log the training progress using the Weights & Biases framework. You can see my logs here."}, {"type": "subtitle", "content": "Evaluating the Model"}, {"type": "sentence", "content": "Evaluating a language generation model is a little more complicated than evaluating something like a classification model. This is because there is no right answer you can compare against like you could with a classification model. The evaluation dataset contains descriptions and the questions that people have asked about those products, but that doesn\u2019t mean that those are the only right questions you can ask."}, {"type": "sentence", "content": "Therefore, one of the best ways to evaluate a language generation model is to generate text and have it evaluated by an actual person (or several people)."}, {"type": "sentence", "content": "Speaking of generating text, impressive developments in decoding algorithms over the past few years has led to models capable of generating quite realistic text sequences. (Decoding algorithms are used to generate text)"}, {"type": "sentence", "content": "The following section gives a brief overview of the popular decoding algorithms currently in use."}, {"type": "subtitle", "content": "Decoding Algorithms"}, {"type": "sentence", "content": "This section is based heavily on the Hugging Face notebook on text generation. I highly recommend going through that notebook to gain a more in-depth understanding of decoding algorithms as it does an excellent job of explaining the algorithms and showing how they can be used."}, {"type": "sentence", "content": "We will be using a combination of both Top-K and Top-p sampling techniques to generate questions with our T5 model. This strategy typically leads to more natural-looking text."}, {"type": "subtitle", "content": "Question Generation"}, {"type": "sentence", "content": "The predict() method of a Simple Transformers T5 model is used to generate the predictions or, in our case, the questions."}, {"type": "sentence", "content": "Here, we are generating 3 questions for each description in the eval_df dataset."}, {"type": "sentence", "content": "Let\u2019s take a look at some of the samples."}, {"type": "sentence", "content": "Just for fun, I\u2019ve shuffled the generated questions with the actual question from the dataset. There are 4 questions for each description, 3 of which are generated and one is the original. See if you can tell which is which! I would love to see your guesses in the comments. \ud83d\ude09"}, {"type": "subtitle", "content": "Sample 1"}, {"type": "sentence", "content": "Description:"}, {"type": "sentence", "content": "Questions:"}, {"type": "subtitle", "content": "Sample 2"}, {"type": "sentence", "content": "Description:"}, {"type": "sentence", "content": "Questions:"}, {"type": "subtitle", "content": "Sample 3"}, {"type": "sentence", "content": "Description:"}, {"type": "sentence", "content": "Questions:"}, {"type": "subtitle", "content": "Sample 4"}, {"type": "sentence", "content": "Description:"}, {"type": "sentence", "content": "Questions:"}, {"type": "subtitle", "content": "Sample 5"}, {"type": "sentence", "content": "Description:"}, {"type": "sentence", "content": "Questions:"}, {"type": "subtitle", "content": "Sample 6"}, {"type": "sentence", "content": "Description:"}, {"type": "sentence", "content": "Questions:"}, {"type": "subtitle", "content": "Bonus sample"}, {"type": "sentence", "content": "Description:"}, {"type": "sentence", "content": "Question:"}, {"type": "sentence", "content": "\ud83d\udc40"}, {"type": "sentence", "content": "You can also test your model on other product descriptions. The script below uses a random description I found on eBay."}, {"type": "sentence", "content": "And the generated questions:"}, {"type": "subtitle", "content": "Wrap Up"}, {"type": "sentence", "content": "For me, the most intriguing aspect of the T5 model is the ability to train it for an entirely new task by merely changing the prefix. In this article, we\u2019ve trained the model to generate questions by looking at product descriptions. However, it is entirely possible to have this same model trained on other tasks and switch between the different tasks by simply changing the prefix."}, {"type": "sentence", "content": "This flexibility opens up a whole new world of possibilities and applications for a T5 model. I can\u2019t wait to see what comes next!"}, {"type": "sentence", "content": "You can also significantly improve the training speed and GPU memory consumption by opting for the t5-base model. This will likely result in comparatively poorer (but by no means poor) performance."}], "topic": "artificial-intelligence"}