{"title": "Building a New Algorithm for Criminal Justice", "data": [{"type": "sentence", "content": "Machine learning has created a new ecosystem in criminal justice. It may just be making things worse."}, {"type": "image", "content": "https://miro.medium.com/max/1400/1*RZ6Sl2JcAKhW_YBmubXZGg.jpeg"}, {"type": "sentence", "content": "The United States criminal justice system is broken. And unfortunately, the use of Big Data is further perpetuating systematic inequities in policing, bail hearings, and sentencing decisions."}, {"type": "sentence", "content": "Predictive policing is defined as the \u201cthe application of analytical techniques \u2014 particularly quantitative techniques \u2014 to identify likely targets for police intervention and prevent crime or solve past crimes by making statistical predictions\u201d. Today it has grown to encompass any use of algorithmic, statistical, or machine-learning based decision-making model aimed at forecasting and mitigating future crime."}, {"type": "sentence", "content": "Although idealistic in nature, even once posed as a panacea to our flawed criminal justice system, predictive policing algorithms are subject to very little federal, state, and local oversight \u2014 often relying on the for- or non-profit organizations that developed them to self-audit."}, {"type": "sentence", "content": "A 2016 study from ProPublica examined over 7,000 criminal arrests in Broward County, Florida. The results casted doubt on the predictive model developed by Northpointe, a non-profit whose tool, COMPAS, is a pioneer in the industry of predictive policing. The study concluded that only 61% of individuals flagged as likely to re-offend were arrested for subsequent crimes within two years \u2014 a small improvement from a coin-flip\u2019s chance. More alarming is that COMPAS falsely flagged black individuals as likely to re-offend at double the rate of false predictions for white individuals."}, {"type": "sentence", "content": "A common pejorative amongst data scientists is \u201cgarbage in, garbage out\u201d. A model using historical data steeped in America\u2019s past of racial inequality will only project a world based on the continuance of that inequity. The model isn\u2019t just predictive, it is generative."}, {"type": "subtitle", "content": "How did we get here?"}, {"type": "sentence", "content": "This story is a convergence of two trends: the first, America\u2019s \u201cWar on Drugs\u201d, and the second, the rise of Big Data and the subsequent loss of our digital privacy. The \u201cWar on Drugs\u201d can be traced back to Lyndon B. Johnson\u2019s presidency, beginning in 1963. Positioned as anti-poverty and pro civil rights, Johnson\u2019s presidency began the modern carceral State in America."}, {"type": "sentence", "content": "The Omnibus Crime Control and Safe Streets Act of 1968 expanded the role of the federal government in local policing through financial assistance and the codification of policies. Instead of addressing the upstream determinants of crime, Johnson\u2019s presidency would use the criminal justice system as an input to building a better society. By the 1970\u2019s, the federal government had instituted harsher punishments for minor offenses and imposed mandatory minimum sentences \u2014 these disproportionately impacted low-level drug offenses. In 1980, approximately 500,000 Americans were incarcerated, by 2000, this had grown to 1.9 million."}, {"type": "sentence", "content": "Meanwhile, technology was beginning to change the way our world operated. In 1970, President Richard M. Nixon signed the Fair Credit Reporting Act. It is still the only federal legislation that gives citizens the right to see the data collected about them, in this case, about their spending habits. Companies like Amazon, Facebook, Netflix, Apple, and Google have gathered data on our locations, spending, interests and other personal details. These for-profit companies have mostly used the data to sell us more products (or monetize the data with third parties), yet, the criminal justice system has much more nefarious objectives."}, {"type": "subtitle", "content": "Legal Framework in America"}, {"type": "sentence", "content": "The federal government has mostly abstained from intervening in issues of data ownership and legality. The majority of algorithms that impact our daily lives are based on fine-print privacy policies that we must opt-in to use. For predictive policing, there is no consent from the \u201cconsumer\u201d. Not only is it permissible for law enforcement to gather data about us, but there is no federal requirement for that data to be made public."}, {"type": "sentence", "content": "In New York state, the Brennan Center has been in an ongoing dispute since 2016 with the New York City Police Department to publicize their predictive policing technologies. The Center recently won that suit, forcing the NYPD to publish details of its algorithm. Most police departments and court systems outsource their modelling to private companies \u2014 these companies have been highly successful in protecting their proprietary predictive algorithms."}, {"type": "sentence", "content": "The first state to limit the powers of predictive modeling in the criminal justice system was Wisconsin in 2016. Wisconsin courts, which had been using computer-generated \u201crisk scores\u201d in sentencing hearings since 2012, can no longer use the scores as a \u201cdeterminative\u201d factor in sentencing decisions. Loomis v. State went to Wisconsin\u2019s supreme court, ruling that sentencing algorithms were not a violation of a citizen\u2019s due process rights, however, risk assessments cannot be determinative and require a printed disclaimer. Loomis appealed and the federal Supreme Court declined to hear the case."}, {"type": "sentence", "content": "Predictive policing will eventually be challenged at the federal level. The nodes of discourse will arise from perceived violations of the 4th, 5th, and 14th amendments \u2014 specifically the probable cause, due process, and equal protection clauses. In 2000\u2019s Illinois v. Wardlow, the Supreme Court ruled that the high-crime nature of a neighborhood can count as one of two factors in elevating an officer\u2019s reasonable suspicion before a stop, impacting enforcement of the 4th amendment."}, {"type": "sentence", "content": "University of the District of Columbia law professor, Andrew Guthrie Ferguson, states that, \u201cIf officers view those individualized and particularized identifying characteristics \u2014 such as prior convictions, gang associations, and GPS coordinates near the scene of the crime \u2014 as suspicious, then otherwise innocent actions might create a predictive composite that satisfies the reasonable suspicion standard.\u201d Big Data has now further reduced the thresholds of reasonable suspicion regarding the 4th amendment."}, {"type": "subtitle", "content": "Analyzing the Problem of Predictive Policing"}, {"type": "sentence", "content": "Predictive policing is clearly flawed, and currently, unregulated. Not only have predictions failed at reducing crime, these models are baked in racial discrimination and have led to over-policing of minority neighborhoods. How can we build a better future that doesn\u2019t just reinforce the mistakes of our past? While some have argued that model failure rates are still an improvement from the historical bias of human judges, there is very little data to confirm this hypothesis. Additionally, the comparison feels irrelevant \u2014 the failure rate is already so high against black individuals that it necessitates immediate structural change, rather than a comparative study."}, {"type": "sentence", "content": "A report from Cornell University found that predictive policing in \u201chigh risk\u201d neighborhoods has created a feedback loop, where officers were constantly returning to the same neighborhoods after making arrests. Similar to a neural network, the outputs became the new inputs. The model had built a future to match its predictions with over-policing \u2014 it had become generative. Any algorithm based primarily on arrests data will be heavily influenced by police bias, rather than actual reported crimes (which tend to be more geographically diverse). With little oversight, America has allowed third party engineers to decide which neighborhoods are policed, who goes to jail, and how long they\u2019re there for. All models are biased and the data is an instrument of control. The goal is to reduce bias as much as possible."}, {"type": "subtitle", "content": "Solving the Problem: A Multi-Pronged Approach"}, {"type": "sentence", "content": "As a society we\u2019ve allowed the owners of data to surveil without any oversight or repercussions. Organizations like AI Now and the ACLU are calling for an end to this panoptic arrangement. AI Now has proposed \u201calgorithmic impact assessments\u201d which would require public agencies to disclose the systems they\u2019re using for outside research analysis and auditing. The ACLU is promoting \u201cCommunity Control over Police Surveillance\u201d that would legally require police departments and other agencies to seek public input prior to using algorithms in the field. These proposals would be a positive first step in allowing researchers and local communities to better understand the local policies affecting their neighborhoods."}, {"type": "sentence", "content": "In courtrooms, defendants rarely have the opportunity to challenge their risk assessments. Julia Angwin\u2019s ProPublica article contains several anecdotal stories of individuals receiving disproportionately high-risk assessment scores due to race or gender \u2014 oftentimes it was the low-scoring individuals who became repeat offenders. If risk assessments are to continue, they need to be fungible and contestable. As a first step, all states should adopt similar legislation as Wisconsin to better educate judges."}, {"type": "sentence", "content": "Predictive models, PredPol and CrimeScan, which are used by police departments across the country, limit their projections to where crimes will occur, rather than who will commit the crime. Meanwhile cities such as Los Angeles and Chicago have built \u201cstrategic subject lists\u201d \u2014 originally designed to monitor and provide resources to potential offenders and victims. However, these lists have mostly devolved into constant monitoring. As a society we must delineate between crime reduction and over-surveillance."}, {"type": "subtitle", "content": "Where Do We Go From Here?"}, {"type": "sentence", "content": "Predictive policing is here to stay in our society, for better or for worse. Evelyn Ruppert, a Data Sociology professor at the University of London, possibly addressed this existential question best in her 2015 essay Who Owns Big Data? The answer is obviously the collector, not us, the dividuals. My question is: should the criminal justice system own our data?"}, {"type": "sentence", "content": "Congress needs to pass federal legislation that would require police departments and outside agencies to publish their data and methods. There needs to be greater accountability at the local level \u2014 especially since these systems are operating with local tax dollars. Additionally, police officers cannot rely on model forecasts to determine reasonable suspicion in making stops. Models should be routinely updated and evaluated for racial discrimination. Finally, judges should only consult risk assessment scores as an ancillary voice in conducting sentencing decisions."}, {"type": "sentence", "content": "The legislative battles have only begun. It is essential that we take advantage of the technological advances in machine learning to build a more equitable criminal justice system. If not, we\u2019ll only repeat the failures of our past."}, {"type": "sentence", "content": "Julian Hartwell is a graduate student at the University of Pennsylvania, studying Social Policy and Data Science. His research focuses on crime, economic mobility, and poverty."}], "topic": "artificial-intelligence"}