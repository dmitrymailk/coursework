{"title": "Handling Highly Imbalanced Datasets In Convolutional Neural Networks", "data": [{"type": "sentence", "content": "In today\u2019s world, Convolutional Neural Networks are being used to serve a lot of purposes. The main tasks achieved by this method are segmentation and classification. Currently, image segmentation is emerging as one of the most important domains for biomedical imaging. Now, in these fields, the models need to be very sure of the results produced as it is a medical field. False negatives i.e, classification of something as not present when it is actually present can land you in a great deal of trouble."}, {"type": "subtitle", "content": "CHALLENGE IN THE FIELD"}, {"type": "sentence", "content": "Now, to operate on these fields sometimes, data imbalance is a major issue. Say, we are applying CNN which basically acts as a Supervised Learning Algorithm is being used for lesion detection. The data set from which the CNN trains and validates has a very very less number of lesion classes compared to a non-lesion class. In other words, the foreground is less compared to the background. This makes the model learn the background in a precise manner due to the abundance of instances but the model fails to learn the foreground that good. It reduces the number of False Positives but increases False Negatives. But, the accuracy is maintained and is actually very high due to the correct classification of the major class i,e the background. This fact can really mislead because actually it is not achieving the actual motive."}, {"type": "image", "content": "https://miro.medium.com/max/2560/1*hy9PZY0cq8_0ihsFAJhyGQ.png"}, {"type": "sentence", "content": "In the image, we can see how the data can be an imbalance. The orange represents the foreground and the blue represents the background. To make sure this does not happen we use performance matrices like Sensitivity and Specificity. Sensitivity is the measure of the True Positive or how good the target class has been learned, here lesion class. Specificity is the measure of the accuracy of learning the background."}, {"type": "sentence", "content": "So these two matrices give a very clear idea of any imbalance that occurs while training. Specificity is penalized by False Positives and Sensitivity by False Negatives."}, {"type": "subtitle", "content": "SOLUTIONS"}, {"type": "sentence", "content": "In Convolutional Neural Networks we usually deal with these kinds of problems using the Loss Function. This problem has remained a challenge for a long time so there are several functions already developed to solve the problems. We will talk about some of those solutions in this article."}, {"type": "subtitle", "content": "Focal Loss"}, {"type": "sentence", "content": "The Focal Loss has been designed to deal with the imbalanced datasets. For example, in Fraud detection where the background i.e, the normal class has many more instances than the foreground or the fraud class. Focal loss is mostly used in these fields."}, {"type": "subtitle", "content": "Custom Loss Functions"}, {"type": "sentence", "content": "In several cases, where there is a high imbalance of data, we can use custom loss function to cope up with the imbalance and try to stabilize the loss function. These approaches usually use a stable loss function established by TensorFlow or Theano. In most cases, they are categorical or binary cross entropies. A wrapper function is used to modify the loss functions returned and used as the loss function in the main model."}, {"type": "sentence", "content": "These approaches are mainly achieved in two ways:"}, {"type": "sentence", "content": "First Method"}, {"type": "sentence", "content": "The original loss function is the Keras loss function used and the weighted list is the list of weights to be applied"}, {"type": "sentence", "content": "This can be assigned to the loss field in the compile method to apply the function. The weights can be obtained by:"}, {"type": "sentence", "content": "This snippet calculates the 0 and 1 split in the target values of the validation set and calculates the ratio. This reflects which of the classes are less represented."}, {"type": "sentence", "content": "This approach simply changes the input set obtained and directly changes the balance of the classes. For instance, if you have 5 samples from class 1 and 10 samples from class 2, pass the samples for class 5 twice in the input arrays."}, {"type": "subtitle", "content": "Second Method"}, {"type": "sentence", "content": "This method tends to apply some weights to the less represented class in the loss function. It modifies the function as:"}, {"type": "sentence", "content": "This works like a weighted loss function which attaches a weight on the less represented class in order to increase the loss, if that class is predicted incorrectly. The weights are obtained in the same manner as earlier."}, {"type": "sentence", "content": "If we are using 2D or 1D models and we do not need to modify weights in compilation time we can use the class weights in the compile method."}, {"type": "sentence", "content": "This method updates the weights every time based on the split of y_true. Initially, it uses the split of y_validate. It uses loss function as binary cross-entropy. We can also replace this function using other established loss functions."}, {"type": "sentence", "content": "This function can be applied using this snippet."}, {"type": "subtitle", "content": "Dice Coefficient Loss"}, {"type": "sentence", "content": "The dice coefficient and Jaccard index are two important performance matrices used in segmentation problems. They are defined as:"}, {"type": "sentence", "content": "Dice Coefficient= 2* TP / (2* TP + FP +FN)"}, {"type": "sentence", "content": "Jaccard Index= TP / (TP + FN +FP)"}, {"type": "sentence", "content": "They focus only on the segmented class. This property of the Dice Coefficient makes it a very good loss function for imbalanced data its performance only depends on how the model recognizes the foreground class. (1-dice coefficient) is known as the Dice Loss"}, {"type": "sentence", "content": "This snippet can be used to use the dice coefficient loss."}, {"type": "subtitle", "content": "Tversky Loss"}, {"type": "sentence", "content": "This is very similar to the Dice loss. It has two parameters that make it a bit customizable from the loss function point of view. The Tversky Index is represented by"}, {"type": "sentence", "content": "(1-Tversky index) is called the Tversky loss function. The two parameters, where mostly (b=1-a) are used to provide some weightage to the represented classes. For example, if a>0.5 we provide weight to the False-negative classification so that the loss function is modified accordingly."}, {"type": "sentence", "content": "This code can be used for Tversky loss. It can be called by using the following snippet."}, {"type": "sentence", "content": "This will help to maintain and increase the dice score metric as it is based on that approach."}, {"type": "subtitle", "content": "Conclusion"}, {"type": "sentence", "content": "These are the loss functions and approaches that are most commonly used to deal with the imbalance CNN data."}], "topic": "data-science"}