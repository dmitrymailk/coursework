{"title": "Entropy Regularization in Reinforcement Learning", "data": [{"type": "sentence", "content": "In our everyday language, we commonly use the term \u201centropy\u201d to refer to the lack of order or predictability of a system (for example, the universe.) In Reinforcement Learning (RL), the term is used in a similar fashion: in RL, entropy refers to the predictability of the actions of an agent. This is closely related to the certainty of its policy about what action will yield the highest cumulative reward in the long run: if certainty is high, entropy is low and vice versa. You can see this in the following images:"}, {"type": "image", "content": "https://miro.medium.com/max/2528/1*xrR-5wnSOlq-aqsAozmxCg.jpeg"}, {"type": "sentence", "content": "The formal definition of entropy in RL has been taken from Information Theory, where entropy is calculated as shown in equation (1), for a discrete random variable x with probability mass function P(X). In RL, the formula becomes equation (2) because we calculate the entropy of the policy \u03c0(a|s_t), where a represents each action, s the state and t the time step. Note that here we use a discrete action space for simplicity, but the definition can easily be applied to continuous action spaces by replacing the sum with an integral."}, {"type": "image", "content": "https://miro.medium.com/max/3530/1*6PkrnQ9OopdVdFY7bHea9A.jpeg"}, {"type": "image", "content": "https://miro.medium.com/max/1860/1*4mPj93YTeFmueHhgaLNbDw.png"}, {"type": "sentence", "content": "If we calculate the entropy for the distributions shown in the first figure, we see how the formula works. In the first distribution of q values, all of the probabilities are similarly low, while in the second one, a_2 has a high probability and the other actions have a low probability. This makes the entropy for the first distribution higher than the entropy for the second distribution, as you can see on the left."}, {"type": "subtitle", "content": "How we use Entropy in RL"}, {"type": "sentence", "content": "When the agent is learning its policy and an action returns a positive reward for a state, it might happen that the agent will always use this action in the future because it knows it produced some positive reward. There could exist another action that yields a much higher reward, but the agent will never try it because it will just exploit what it has already learned. This means the agent can get stuck in a local optimum because of not exploring the behavior of other actions and never finding the global optimum."}, {"type": "sentence", "content": "This is where entropy comes handy: we can use entropy to encourage exploration and avoid getting stuck in local optima. To formalise this, we augment the conventional RL objective with the entropy of the policy as in Ziebart (2010). The maximum-entropy RL objective is then defined as:"}, {"type": "image", "content": "https://miro.medium.com/max/3692/1*cyyLc0n9zPE4i7H7TFNzoA.jpeg"}, {"type": "sentence", "content": "The idea of learning such maximum entropy model has its origin in statistical modeling, in which the goal is to find the probability distribution that has the highest entropy while still satisfying the observed statistics [Tang & Haarnoja (2017)]. The principle of maximum entropy states that the probability distribution with the highest entropy, is the one that best represents the current state of knowledge in the context of precisely stated prior data (in our case, these stated prior data is the experience of the agent.)"}, {"type": "sentence", "content": "We now calculate the q values using an entropy bonus, which means that we now add the entropy H[\u03c0(a|s_t)] to our q value. In soft Q-learning, Haarnoja et al. (2017) incorporate entropy with the following equation:"}, {"type": "image", "content": "https://miro.medium.com/max/3814/1*AWsBLd1K_DlOLYaJ333RhA.jpeg"}, {"type": "subtitle", "content": "Why we use Entropy in RL"}, {"type": "sentence", "content": "Entropy has quickly become a popular regularization mechanism in RL. In fact, many of the current state-of-the-art RL approaches such as Soft Actor-Critic, A3C and PPO, use it for multiple benefits:"}, {"type": "subtitle", "content": "Improved Exploration"}, {"type": "sentence", "content": "As we said before, entropy encourages exploration, avoiding situations in which the agent might fall into a local optimum. This is very important for tasks with sparse reward because the agent does not receive feedback of its actions often and might therefore \u201coverestimate\u201d some reward received and always repeat the actions that led to that reward."}, {"type": "subtitle", "content": "Fine-tuning Policies"}, {"type": "sentence", "content": "Indirectly, encouraging exploration also facilitates transferring learning from a learned policy to a new one. For example, if we train a robot to walk an area, when we put this robot in a maze, the robot can re-utilize its knowledge of walking to navigate the maze, as opposed to a robot that starts from scratch, without any knowledge. If we use a regular policy \u2014 without any use of entropy \u2014 the agent will take longer to adapt to the new task, because it has already learned what yielded reward before and will not explore as much as an agent that uses a maximum-entropy policy. This is visible in the following video:"}, {"type": "subtitle", "content": "More Robustness"}, {"type": "sentence", "content": "Since the agent will explore more states while learning, \u2014 thanks to the encouraged exploration of its maximum-entropy policy \u2014 the agent will also be more robust to abnormal or rare events while developing a task. This makes the agent more robust, because it will know how to deal better in different situations."}, {"type": "subtitle", "content": "Conclusion"}, {"type": "sentence", "content": "The application of entropy in RL has brought many benefits: it improves the exploration of the agent, it lets us fine-tune policies that were previously used for different tasks and are also more robust to rare states of the environment. Because of this, it has become very popular in the design of RL approaches such as Soft Actor-Critic, A3C and others."}, {"type": "sentence", "content": "Its effects can vary greatly on the environment it is applied though, and therefore it is worth checking to see if entropy is really a benefit for your RL setting or not. If you want to go deeper into the topic, I recommend these two publications that provide a detailed analysis of entropy regularization in RL (I have used them among other materials to write this article):"}, {"type": "sentence", "content": "Thanks for reading! :)"}], "topic": "data-science"}