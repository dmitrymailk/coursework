{"title": "Disk Caching using Joblib", "data": [{"type": "subtitle", "content": "Easy and fast result caching for machine learning pipelines"}, {"type": "sentence", "content": "Let\u2019s begin our journey of developing machine learning pipelines using Joblib. In this article, we will see a \u201ccomparative study of result caching with Joblib\u201d along with code."}, {"type": "subtitle", "content": "Outline"}, {"type": "sentence", "content": "Feel free to jump to any section you like, but I recommend reading the complete article. It will hardly take 10 minutes."}, {"type": "subtitle", "content": "Market Need"}, {"type": "sentence", "content": "In our fast-growing world, as the data is growing exponentially, so is the computational time of applying scientific methods to it. Data processing pipelines, undoubtedly, are an essential part of any machine learning project. There is a massive demand in the market today for faster and reliable data processing pipelines. Moreover, organizations expect data scientists to have some data engineering skills, like knowing tools to build quick and dirty data processing pipelines for POCs, in their arsenal."}, {"type": "subtitle", "content": "Ways of reducing computational time"}, {"type": "sentence", "content": "There are primarily two ways of reducing the total processing time of a pipeline:"}, {"type": "sentence", "content": "We can avoid re-computing results of computationally intensive functions and cache them on disk to save time. Thus, the next time when we invoke the function with the same input arguments, instead of re-computing the results, cached results will be used."}, {"type": "sentence", "content": "We can distribute independent tasks (e.g., applying the same transformation to our data points) to run in parallel on multiple GPUs, CPUs, or threads. Parallelization helps us by reducing the overall computational time if we utilize our machines optimally."}, {"type": "sentence", "content": "Caching results and parallelizing functions in a pipeline may seem complicated and time-consuming, but with Joblib, it becomes a child\u2019s play. Joblib provides easy and smooth functions for building lightweight pipelines in Python."}, {"type": "subtitle", "content": "Why use Joblib?"}, {"type": "sentence", "content": "Few of the reasons why I like Joblib are:"}, {"type": "subtitle", "content": "Ways of caching the result in Joblib"}, {"type": "sentence", "content": "Let\u2019s dive into code on how to cache results using Joblib."}, {"type": "subtitle", "content": "Install the package"}, {"type": "sentence", "content": "pip install joblib"}, {"type": "subtitle", "content": "Importing and initialization"}, {"type": "subtitle", "content": "Define some large input"}, {"type": "subtitle", "content": "Caching function results: Our primary goal"}, {"type": "sentence", "content": "Memory class defines a method cache for caching our function. There are two ways to pass a function to Memory.cache"}, {"type": "sentence", "content": "Method 1 \u2014 Passing a function directly to Memory.cache"}, {"type": "sentence", "content": "Define our function"}, {"type": "sentence", "content": "Let\u2019s call the function with our input and record the execution time."}, {"type": "image", "content": "https://miro.medium.com/max/2514/1*GpHPQ6lT5M6d4MHFgUMnug.png"}, {"type": "sentence", "content": "Let\u2019s call the function with same input"}, {"type": "image", "content": "https://miro.medium.com/max/2872/1*BTBN_AbPpTSI5vONwxs5XA.png"}, {"type": "sentence", "content": "If you look at the output above, you can see that the print statements were not executed. This is because, the cached result was returned instead of re-computing the result."}, {"type": "sentence", "content": "Note the time difference in execution. The execution time got reduced by 6x. That\u2019s the power of caching!"}, {"type": "sentence", "content": "Method 2\u2014 Memory.cache as decorator"}, {"type": "sentence", "content": "Define our function"}, {"type": "sentence", "content": "Let\u2019s call the function with our input and record the execution time."}, {"type": "image", "content": "https://miro.medium.com/max/2498/1*rwNZQJycDM-1M-e3KGmofg.png"}, {"type": "sentence", "content": "Let\u2019s call the function with same input again"}, {"type": "image", "content": "https://miro.medium.com/max/2444/1*z-IXN5uT6itNlI0Z9_mS4Q.png"}, {"type": "sentence", "content": "Note the difference in the execution time of before and after caching."}, {"type": "sentence", "content": "Important \u2014 Did you know that we can go faster than this \ud83d\ude31. Keep on reading!"}, {"type": "subtitle", "content": "Faster cache lookup \u2014 reducing total execution time further"}, {"type": "sentence", "content": "Good News: We can further reduce total execution time by reducing cache lookup"}, {"type": "sentence", "content": "In deep learning, data mostly comes in as NumPy arrays whether you are working on computer vision, time series, or natural language processing problems. A smart way to reduce time is to use memmap (Memory map) in NumPy. Memory map speeds cache lookups when reloading large NumPy arrays."}, {"type": "sentence", "content": "We don\u2019t need to make any significant change if we want to use Memory map in Joblib. Just pass mmap_mode argument to the constructor of the Memory class while creating its object, and that\u2019s it."}, {"type": "sentence", "content": "Let\u2019s see some quick code"}, {"type": "sentence", "content": "Let\u2019s call the function with our input and record the execution time."}, {"type": "image", "content": "https://miro.medium.com/max/2580/1*hermd4CQXYJDxer1MxgyOQ.png"}, {"type": "sentence", "content": "Let\u2019s call the function with our input again."}, {"type": "image", "content": "https://miro.medium.com/max/2196/1*SCZMhnPdseigqwbUuKLM7A.png"}, {"type": "sentence", "content": "Note the difference in the execution time of before and after caching."}, {"type": "subtitle", "content": "Clearing cache"}, {"type": "sentence", "content": "While building our pipeline, we sometimes might need to clear the cache of a particular function or the complete cache directory."}, {"type": "subtitle", "content": "Clearing cache for individual function"}, {"type": "sentence", "content": "We will clear cache for the already defined functions func_mem and func_as_decor"}, {"type": "subtitle", "content": "Clear cache for all the functions"}, {"type": "sentence", "content": "Instead of clearing the cache for individual functions, we can also clear the cache for all the functions in one go."}, {"type": "subtitle", "content": "Summary"}, {"type": "sentence", "content": "In this article, we saw how we could use Joblib for easy, fast, and efficient caching."}, {"type": "sentence", "content": "The following table is a comparison between different function caching methods or styles in Joblib. It uses execution time as the metric for evaluation."}, {"type": "sentence", "content": "Keynotes"}, {"type": "subtitle", "content": "Before you go"}, {"type": "subtitle", "content": "Complete code link"}, {"type": "sentence", "content": "You can check out the complete code on Kaggle: Disk Caching using Joblib."}, {"type": "subtitle", "content": "Next in the line"}, {"type": "sentence", "content": "In the next article, we will see how to parallelize independent tasks in Joblib for faster execution time."}, {"type": "subtitle", "content": "Feedback"}, {"type": "sentence", "content": "I thank you for your time for reading this article. I have tried to make it as informative and code-oriented as possible. If you liked the article, share it with your friends and colleagues. Comment below if you would like to share something. Your feedback is highly valued and appreciated."}, {"type": "subtitle", "content": "Connect with me"}, {"type": "sentence", "content": "Follow me on Medium to get my latest articles and projects."}, {"type": "sentence", "content": "Connect on other platforms \ud83d\ude42 :"}], "topic": "data-science"}