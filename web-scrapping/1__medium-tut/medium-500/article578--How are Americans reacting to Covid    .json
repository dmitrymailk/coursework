{"title": "How are Americans reacting to Covid-19?", "data": [{"type": "subtitle", "content": "Using Twitter and sentiment analysis to answer the question"}, {"type": "image", "content": "https://miro.medium.com/max/1240/1*G7hhNbwwhrbLlfTw5IPSSg.jpeg"}, {"type": "sentence", "content": "The Covid-19 pandemic poses an unprecedented challenge to the entire world. With the most confirmed cases and deaths, America is one of the countries hardest hit by the virus. As states begin to partially reopen, the nation has become very polarized on the topic. Some firmly advocate for this measure, citing the importance of the country\u2019s economic health. Yet, others have a strong objection to this, contending that the human cost of reopening can\u2019t be justified. At a time when tensions are high, I sought to gain a better understanding of how exactly Americans feel about the current state of affairs surrounding Covid-19."}, {"type": "sentence", "content": "In an attempt to answer this question, Srihan Mediboina and I worked together to scrape tweets related to Covid-19 from Twitter and perform sentiment analysis on them. To find out how reactions varied across America, we used tweets from New York, Texas, and California. Let\u2019s get into the project!"}, {"type": "subtitle", "content": "Getting Twitter Data"}, {"type": "image", "content": "https://miro.medium.com/max/3420/1*UKiOVwTLOHADu_Lk13nkaQ.png"}, {"type": "sentence", "content": "Before we can access our Twitter API credentials, we need to apply for a Twitter developer account. Once our application is approved, we can use Tweepy to access the API and download all the tweets for a hashtag. Calling the search_for_hashtag function allows us to quickly scrape data across hashtags (#coronavirus, #Covid-19, #NewYork, #California, #Texas were some of the hashtags we used). For a more in-depth look at Tweepy, check out this article."}, {"type": "sentence", "content": "We performed the sentiment analysis using a Naive Bayes classifier, which requires labeled data because it\u2019s a supervised learning algorithm. Thus, we manually labeled 500 tweets from each of the three states for a total of 1,500 tweets. Each tweet received either a -1 for negative sentiment, a 0 for neutral sentiment, or a 1 for positive sentiment. If you\u2019re interested in performing your own analysis, here\u2019s a link to the data."}, {"type": "image", "content": "https://miro.medium.com/max/1764/1*auhtVdWnPEAEiQZkI61gKA.png"}, {"type": "subtitle", "content": "Tokenizing"}, {"type": "sentence", "content": "Now we tokenize the tweets by splitting them into individual words (called tokens). Without tokens, we can\u2019t carry out the subsequent steps involved in sentiment analysis. This process becomes simple when we import TweetTokenizer from the Natural Language Toolkit (nltk). The tokenize_tweets function is only two lines of code and we can apply it to the dataframes to break up the tweets. nltk is a very powerful package for sentiment analysis so we\u2019ll be using it throughout the article."}, {"type": "image", "content": "https://miro.medium.com/max/1764/1*4leDTGc5h4E1eO3udJ0A9w.png"}, {"type": "subtitle", "content": "Stopwords"}, {"type": "sentence", "content": "Stopwords are common words such as \u201cthe\u201d, \u201ca\u201d, and \u201can\u201d. Since these words don\u2019t further our understanding of the sentiment of the text, we filter them out. By importing stopwords from ntlk, this step becomes pretty simple: the remove_stopwords function is also two lines of code."}, {"type": "image", "content": "https://miro.medium.com/max/1764/1*WarAUl_0f3UZHBzwJQAweA.png"}, {"type": "sentence", "content": "Some of the stopwords that were removed from the first few rows of our California dataset include \u201csome\u201d, \u201ccan\u201d, \u201cjust\u201d, and \u201cfor\u201d."}, {"type": "subtitle", "content": "Cleaning Text"}, {"type": "sentence", "content": "In addition to removing stopwords, we want to make sure that any random characters in our data frames are also removed. For example, several characters such as \u2018x97\u2019 and \u2018xa3\u2019 appeared in the csv files after we scraped the tweets. After iterating through to find these miscellaneous characters, we copy pasted them into the CleanTxt function. Then, we applied the function to each data frame to remove them."}, {"type": "image", "content": "https://miro.medium.com/max/1764/1*QAZWG8Tf-bsx-bJ0q54j9w.png"}, {"type": "sentence", "content": "As we can see, hashtags were the most prevalent characters that were removed. By cleaning the text, we can improve our model\u2019s performance."}, {"type": "subtitle", "content": "Lemmatizing"}, {"type": "sentence", "content": "Often, words referring to the same thing appear in different forms (ex. trouble, troubling, troubled, and troubles all essentially refer to trouble). By lemmatizing the text, we group various inflections of a word together to analyze them as the word\u2019s lemma (how it appears in the dictionary). This process prevents the computer from mistaking different forms of a word for different words. We import WordNetLemmatizer fromnltk and call the lemmatize_tweets function for this purpose."}, {"type": "subtitle", "content": "Master Dataset"}, {"type": "sentence", "content": "Since we\u2019re done with the preprocessing steps, we can move onto creating a master dataset which encompasses all 1,500 tweets. By using df.itertuples, we can iterate over dataframe rows as tuples to append the \u2018tweet text\u2019and \u2018values\u2019 attributes to our dataset. Then, we shuffle our dataset using random.shuffle to prevent our model from falling victim to overfitting."}, {"type": "sentence", "content": "Following that step, we iterate through all of the data frames and add every single word to the all_words list. Next, we use nltk.FreqDist to create a distribution of the frequency of each word. Since some words are more common than others, we want to ensure that the most relevant words are used to train our Naive Bayes classifier. Currently, each tweet is a list of words. However, we can represent each tweet as a dictionary instead of a list: the keys are the word features and the values are either True or False based on if the tweet contains that word feature. This dictionary representing the tweets is known as a feature set. We will generate feature sets for each tweet and train our Naive Bayes classifier on the feature sets."}, {"type": "subtitle", "content": "Training/Testing the Model"}, {"type": "sentence", "content": "The feature_sets will be split 80/20 into the training and testing set, respectively. After training the Naive Bayes classifier on the training set, we can check its performance by comparing its predictions for the sentiment of tweets (results[i]) against the labeled sentiment of the tweets (testing_set[i][0])."}, {"type": "image", "content": "https://miro.medium.com/max/1048/1*0FzWzT68Nr5TqWyajKTSjA.png"}, {"type": "sentence", "content": "Our output shows the predicted value on the left and the actual value on the right. The error percentage of 40% is very high, which translates to our model only being accurate around 3 out of 5 times. Some improvements that can make our model more accurate are using a larger training set or using a validation set to test out different models before selecting the most efficient."}, {"type": "subtitle", "content": "Using the Model"}, {"type": "sentence", "content": "With the model trained/tested, we can use it now to make predictions on a fresh batch of tweets. We scrape more tweets and run through the same preprocessing steps as we did before for the new dataframes: ca_new_df, ny_new_df, and tx_new_df. The predictions of our classifier are stored in results_new_ca, results_new_ny, and results_new_tx. Our last step is to use the sentiment_percent function to quantify the percentages."}, {"type": "image", "content": "https://miro.medium.com/max/888/1*0YGzJyG7Ra5wEtjbFBzvbQ.png"}, {"type": "image", "content": "https://miro.medium.com/max/820/1*dgT90lXSf02i9GyBwnHhTg.png"}, {"type": "image", "content": "https://miro.medium.com/max/828/1*C62qRD5NOtbY4kx72hxVGA.png"}, {"type": "sentence", "content": "In our results, California had only around 6% of tweets as positive while Texas had around 27% of tweets as negative. California and New York both had 73% of tweets neutral with their positive and negative percentages varying by around 4%. Texas did have the most percent of tweets negative, but they also had the most amount of tweets positive at around 10% because a lower percentage of their tweets were neutral. It\u2019s important to keep in mind that our model was only 60% accurate so these results probably aren\u2019t the most indicative of the real sentiment expressed in these tweets."}, {"type": "sentence", "content": "Some code was omitted from this article for the sake of brevity. Click here for the full code."}, {"type": "subtitle", "content": "References"}, {"type": "sentence", "content": "[1] Computer Science channel, Twitter Sentiment Analysis Using Python, Youtube"}, {"type": "sentence", "content": "[2] Vicky Qian, Twitter Crawler, Github"}, {"type": "sentence", "content": "[3]Mohamed Afham, Twitter Sentiment Analysis using NLTK, Python, Towards Data Science"}, {"type": "sentence", "content": "[4]Adam Majmudar, Machines\u2019 key to understanding humans, Medium"}, {"type": "sentence", "content": "Thanks for reading the article! I\u2019m Roshan, a 16 year old that\u2019s super passionate about the various applications of AI. I worked closely on this project with Srihan Mediboina, another teen with a strong interest in AI."}, {"type": "sentence", "content": "Reach out to us on Linkedin:"}, {"type": "sentence", "content": "https://www.linkedin.com/in/roshan-adusumilli/"}, {"type": "sentence", "content": "https://www.linkedin.com/in/srihanmediboina/"}], "topic": "data-science"}